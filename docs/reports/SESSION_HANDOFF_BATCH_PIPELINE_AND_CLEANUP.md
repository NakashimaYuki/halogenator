# Session Handoff: Batch Pipeline Debugging & Repository Cleanup

**æ—¥æœŸ:** 2025-12-29
**çŠ¶æ€:** ğŸŸ¡ Pipelineéƒ¨åˆ†è¿è¡Œï¼Œéœ€è°ƒæŸ¥æ·±å±‚é—®é¢˜å¹¶å®Œæˆæ¸…ç†
**ä¸Šä¸€ä¼šè¯:** å‚æ•°ä¼˜åŒ–ä¸æ‰¹å¤„ç†pipelineå®æ–½
**æœ¬ä¼šè¯æ ¸å¿ƒä»»åŠ¡:**
1. è°ƒæŸ¥batch pipeline timeoutçš„æ ¹æœ¬åŸå› 
2. å®Œæ•´æ¸…ç†å’Œæ•´ç†repository
3. å‡†å¤‡Linuxè¿ç§»

---

## æ‰§è¡Œæ‘˜è¦

### å·²å®Œæˆçš„é‡è¦å·¥ä½œ

1. âœ… **å‚æ•°ä¼˜åŒ–Grid Search** - å®Œæˆ4ä¸ªé…ç½®æµ‹è¯•ï¼Œè¯†åˆ«æœ€ä¼˜é…ç½®
2. âœ… **æ‰¹å¤„ç†Pipelineè®¾è®¡ä¸å®ç°** - åˆ›å»ºè‡ªåŠ¨åŒ–åˆ†æ‰¹å¤„ç†ç³»ç»Ÿ
3. âœ… **éƒ¨åˆ†æ•°æ®å¤„ç†** - æˆåŠŸå®Œæˆ2/14 chunks

### å½“å‰çŠ¶æ€

**Pipelineè¿è¡Œæƒ…å†µ:**
- Chunk 0: âœ… å®Œæˆ (44åˆ†é’Ÿ, 274MBè¾“å‡º)
- Chunk 1: âœ… å®Œæˆ (73åˆ†é’Ÿ, 2.47M products, 190MB)
- Chunk 2: âŒ Timeoutå¤±è´¥ (>4å°æ—¶, 7.31M products, 671MB **éƒ¨åˆ†è¾“å‡º**)
- Chunks 3-13: â¸ï¸ å¾…å¤„ç†

**å…³é”®é—®é¢˜:**
- Chunkå¤„ç†æ—¶é—´å·®å¼‚å·¨å¤§ï¼ˆ44åˆ†é’Ÿ vs >4å°æ—¶ï¼‰
- å¯èƒ½å­˜åœ¨æ¯”timeoutæ›´æ·±å±‚çš„ç³»ç»Ÿæ€§é—®é¢˜
- Repositoryéœ€è¦æ•´ç†ä»¥ä¾¿è¿ç§»åˆ°Linux

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šå·²å®Œæˆä»»åŠ¡æ€»ç»“

### 1.1 å‚æ•°ä¼˜åŒ–Grid Search

**æ–‡ä»¶:** `E:\Projects\halogenator\optimize_parameters.py`

**æµ‹è¯•ç»“æœ:** (150K rowsæµ‹è¯•é›†)

| Config | Workers | Max-In-Flight | Batch | Throughput | Memory | Result |
|--------|---------|---------------|-------|------------|--------|--------|
| Test 1 | 16 | 6 | 50K | 672.5 mol/s | 86.7% | âŒ å†…å­˜è¶…é™ |
| Test 2 | 16 | 8 | 50K | 661.1 mol/s | 85.7% | âŒ å†…å­˜è¶…é™ |
| Test 3 | 16 | 6 | 25K | 798.9 mol/s | 76.6% | âš ï¸ æ¥è¿‘é™åˆ¶ |
| Test 4 | 12 | 8 | 50K | 657.7 mol/s | 43.8% | âœ… **æœ€å®‰å…¨** |

**å…³é”®å‘ç°:**
- 150Kè§„æ¨¡æµ‹è¯•å…¨éƒ¨æˆåŠŸ
- 500K+è§„æ¨¡æµ‹è¯•**å…¨éƒ¨å¤±è´¥**ï¼ˆworkerså´©æºƒï¼‰
- ç»“è®ºï¼šéœ€è¦åˆ†æ‰¹å¤„ç†ç­–ç•¥

**è¾“å‡ºæ–‡ä»¶:**
- `optimization_results_20251228_041629.csv` - è¯¦ç»†æµ‹è¯•æ•°æ®

### 1.2 æ‰¹å¤„ç†Pipelineå®ç°

**æ–‡ä»¶:** `E:\Projects\halogenator\batch_transform_pipeline.py`

**è®¾è®¡ç‰¹æ€§:**
- âœ… CheckpointåŠŸèƒ½ - æ–­ç‚¹ç»­ä¼ 
- âœ… è‡ªåŠ¨çŠ¶æ€è¿½è¸ª - `pipeline_state.json`
- âœ… å¤±è´¥æ¢å¤ - åªé‡è·‘å¤±è´¥chunks
- âœ… è‡ªåŠ¨ç»“æœåˆå¹¶

**é…ç½®å‚æ•°:**
```python
input_file: polyphenol-2X/products.parquet (13.79M rows)
chunk_size: 1,000,000 rows â†’ 14 chunks
workers: 16
max_in_flight: 6
batch_size: 50,000
timeout: 14400s (4å°æ—¶) â† å½“å‰è®¾ç½®
```

**è¾“å‡ºä½ç½®:**
- ä¸»ç›®å½•: `data/output/transforms/polyphenol-2X_BATCHED/`
- Chunks: `data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_XXX_output/`
- çŠ¶æ€æ–‡ä»¶: `data/output/transforms/polyphenol-2X_BATCHED/pipeline_state.json`

### 1.3 å®é™…æ‰§è¡Œç»“æœ

**Chunk 0è¯¦æƒ…:**
```json
{
  "elapsed_seconds": 2652 (44.2 min),
  "output_file": "274 MB",
  "throughput": 1476.7 mol/s,
  "unique_products": æœªè®°å½•ï¼ˆJSONè§£æé—®é¢˜ï¼‰
}
```

**Chunk 1è¯¦æƒ…:**
```json
{
  "elapsed_seconds": 4382 (73.0 min),
  "output_file": "190 MB",
  "throughput": 994.7 mol/s,
  "unique_products": 2,465,730
}
```

**Chunk 2è¯¦æƒ…ï¼ˆå…³é”®å¼‚å¸¸ï¼‰:**
```
Status: timeout failed
Actual output: 671 MB (æœ€å¤§ï¼)
Flush count: #3650
Products written: 7,307,108 (æ˜¯chunk 1çš„3å€ï¼)
Last activity: 06:40 (è¢«4å°æ—¶timeoutç»ˆæ­¢)
Memory: 45.7% (ç¨³å®šå®‰å…¨)
Missing: SUMMARY.json (æœªæ­£å¸¸å®Œæˆ)
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šå…³é”®é—®é¢˜çš„æ·±åº¦åˆ†æ

### 2.1 Timeouté—®é¢˜çš„è¡¨é¢ç°è±¡

**ç°è±¡:**
- Chunk 0: 44åˆ†é’Ÿå®Œæˆ
- Chunk 1: 73åˆ†é’Ÿå®Œæˆï¼ˆæ…¢65%ï¼‰
- Chunk 2: >4å°æ—¶æœªå®Œæˆï¼ˆæ…¢>5å€ï¼‰

**æ•°æ®é‡å¯¹æ¯”:**
- Chunk 0: 274MB â†’ æ¨æµ‹~3-4M products
- Chunk 1: 190MB â†’ 2.47M products
- Chunk 2: 671MB â†’ 7.31M products âš ï¸

**å•ä½æ—¶é—´äº§å‡º:**
- Chunk 0: 1476 mol/s
- Chunk 1: 995 mol/s (æ…¢33%)
- Chunk 2: <500 mol/s? (æ¨ç®—ï¼Œæœªå®Œæˆ)

### 2.2 å¯èƒ½çš„æ·±å±‚æ¬¡åŸå› ï¼ˆéœ€éªŒè¯ï¼‰

#### å‡è®¾1ï¼šåˆ†å­å¤æ‚åº¦æ¢¯åº¦åˆ†å¸ƒ ğŸ” **ä¼˜å…ˆè°ƒæŸ¥**

**é—®é¢˜:** polyphenol-2Xæ•°æ®é›†å¯èƒ½æŒ‰æŸç§ç‰¹å¾æ’åºï¼Œå¯¼è‡´ï¼š
- å‰1M (chunk 0): ç®€å•åˆ†å­ï¼Œå°‘phenolic OH
- ä¸­1M (chunk 1): ä¸­ç­‰å¤æ‚åº¦
- åæ®µ (chunk 2+): é«˜åº¦å¤æ‚ï¼Œå¤šphenolic OH â†’ æŒ‡æ•°çº§è¡ç”Ÿç‰©

**éªŒè¯æ–¹æ³•:**
```python
# æ£€æŸ¥å„chunkçš„åˆ†å­ç‰¹å¾åˆ†å¸ƒ
import pyarrow.parquet as pq
from rdkit import Chem
from rdkit.Chem import Descriptors

for i in range(3):
    chunk_file = f"data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_{i:03d}_input.parquet"
    table = pq.read_table(chunk_file)
    smiles_list = table['smiles'].to_pylist()[:1000]  # é‡‡æ ·1000ä¸ª

    phenolic_oh_counts = []
    for smi in smiles_list:
        mol = Chem.MolFromSmiles(smi)
        if mol:
            # ç»Ÿè®¡phenolic OHæ•°é‡
            pattern = Chem.MolFromSmarts('[OH][c]')
            matches = mol.GetSubstructMatches(pattern)
            phenolic_oh_counts.append(len(matches))

    print(f"Chunk {i}:")
    print(f"  Avg phenolic OH: {sum(phenolic_oh_counts)/len(phenolic_oh_counts):.2f}")
    print(f"  Max phenolic OH: {max(phenolic_oh_counts)}")
    print(f"  åˆ†å­é‡åˆ†å¸ƒ: {[Descriptors.MolWt(Chem.MolFromSmiles(s)) for s in smiles_list[:100]]}")
```

**å¦‚æœéªŒè¯æˆç«‹ï¼Œè§£å†³æ–¹æ¡ˆ:**
- éšæœºæ‰“ä¹±chunksé¡ºåº
- æˆ–æ ¹æ®å¤æ‚åº¦é¢„åˆ†ç±»ï¼Œå¹³è¡¡åˆ†é…
- æˆ–åŠ¨æ€è°ƒæ•´æ¯chunkçš„rowæ•°

#### å‡è®¾2ï¼šå†…å­˜ç¢ç‰‡åŒ–ç´¯ç§¯ ğŸ” **æ¬¡ä¼˜å…ˆ**

**é—®é¢˜:**
- RDKit C++å¯¹è±¡é¢‘ç¹åˆ›å»º/é”€æ¯
- Python GCä¸C++ memoryä¸åŒæ­¥
- é•¿æ—¶é—´è¿è¡Œåå†…å­˜ç¢ç‰‡åŒ–ä¸¥é‡

**éªŒè¯æ–¹æ³•:**
```bash
# ç›‘æ§chunkå¤„ç†è¿‡ç¨‹ä¸­çš„å†…å­˜è¶‹åŠ¿
grep "memory:" chunk_002_output/transform.log | awk '{print $NF}' | sed 's/%//' > memory_trend.txt

# åˆ†ææ˜¯å¦æœ‰å†…å­˜æ³„æ¼æˆ–ç¢ç‰‡åŒ–
python -c "
import matplotlib.pyplot as plt
import numpy as np
data = np.loadtxt('memory_trend.txt')
plt.plot(data)
plt.xlabel('Flush Number')
plt.ylabel('Memory %')
plt.title('Chunk 2 Memory Trend')
plt.savefig('chunk2_memory_trend.png')
print(f'Memory start: {data[0]:.1f}%')
print(f'Memory end: {data[-1]:.1f}%')
print(f'Memory drift: {data[-1] - data[0]:.1f}%')
"
```

**å¦‚æœå­˜åœ¨å†…å­˜æ¼‚ç§»ï¼Œè§£å†³æ–¹æ¡ˆ:**
- å®šæœŸé‡å¯worker pool
- å‡å°‘max-in-flighté™ä½å¹¶å‘å‹åŠ›
- å¢åŠ explicit gc.collect()

#### å‡è®¾3ï¼šWindows ProcessPoolExecutoré™åˆ¶ ğŸ” **éœ€Linuxå¯¹æ¯”**

**é—®é¢˜:**
- Windows multiprocessingå·²çŸ¥æœ‰é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§é—®é¢˜
- Process serialization overhead
- File handleæ³„æ¼

**éªŒè¯æ–¹æ³•:**
```bash
# åœ¨Linuxä¸Šè¿è¡Œç›¸åŒé…ç½®çš„chunk 2æµ‹è¯•
# å¯¹æ¯”æ‰§è¡Œæ—¶é—´å’Œç¨³å®šæ€§
```

**å¦‚æœæ˜¯Windowsç‰¹å®šé—®é¢˜:**
- è¿ç§»åˆ°LinuxæœåŠ¡å™¨ï¼ˆå·²åœ¨è®¡åˆ’ä¸­ï¼‰
- æˆ–ä½¿ç”¨å•è¿›ç¨‹mode (workers=1)

#### å‡è®¾4ï¼šI/Oç“¶é¢ˆ

**é—®é¢˜:**
- 671MBæ–‡ä»¶å†™å…¥æ˜¯å¦å¯¼è‡´I/Oé˜»å¡
- Parquetå†™å…¥æ€§èƒ½ä¸‹é™

**éªŒè¯æ–¹æ³•:**
```bash
# ç›‘æ§I/O wait during chunk processing
# Windows: Resource Monitor -> Disk activity
# Linux: iostat -x 5
```

**è§£å†³æ–¹æ¡ˆ:**
- ä½¿ç”¨SSDå­˜å‚¨
- å¢åŠ flush_intervalå‡å°‘å†™å…¥é¢‘ç‡

### 2.3 è¯Šæ–­è®¡åˆ’ï¼ˆä¸‹ä¸€ä¼šè¯æ‰§è¡Œï¼‰

**Step 1: æ•°æ®ç‰¹å¾åˆ†æï¼ˆ30åˆ†é’Ÿï¼‰**
```bash
cd E:/Projects/halogenator
python scripts/diagnose_chunk_complexity.py  # éœ€åˆ›å»ºæ­¤è„šæœ¬
```

**Step 2: å†…å­˜è¶‹åŠ¿åˆ†æï¼ˆ10åˆ†é’Ÿï¼‰**
```bash
python scripts/analyze_memory_trend.py chunk_002_output/transform.log
```

**Step 3: å•chunkæ·±åº¦æµ‹è¯•ï¼ˆ2å°æ—¶ï¼‰**
```bash
# åœ¨é«˜verbosityä¸‹é‡è·‘chunk 2ï¼Œç›‘æ§æ‰€æœ‰æŒ‡æ ‡
python batch_transform_pipeline.py --single-chunk 2 --verbose
```

**Step 4: Linuxå¯¹æ¯”æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰**
```bash
# åœ¨Linuxä¸Šè¿è¡Œç›¸åŒæµ‹è¯•
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¾…å®Œæˆä»»åŠ¡è¯¦ç»†è¯´æ˜

### ä»»åŠ¡Aï¼šæ·±å…¥è°ƒæŸ¥Timeoutæ ¹æœ¬åŸå›  ğŸ¯ **æœ€é«˜ä¼˜å…ˆçº§**

**ç›®æ ‡:**
1. ç¡®è®¤chunk 2+å¤„ç†æ…¢çš„æ ¹æœ¬åŸå› 
2. æ‰¾åˆ°ç¨³å®šå¯é çš„è§£å†³æ–¹æ¡ˆ
3. é¿å…ç®€å•ç²—æš´çš„"å¢åŠ timeout"

**å®æ–½æ­¥éª¤:**

#### A1. åˆ›å»ºè¯Šæ–­å·¥å…·

**æ–‡ä»¶:** `scripts/diagnose_chunk_complexity.py`

```python
#!/usr/bin/env python3
"""
Diagnose chunk complexity distribution to understand timeout issues.
"""

import pyarrow.parquet as pq
from rdkit import Chem
from rdkit.Chem import Descriptors
import numpy as np
from pathlib import Path

def analyze_chunk_complexity(chunk_id, sample_size=1000):
    """Analyze molecular complexity in a chunk."""

    chunk_file = Path(f"data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_{chunk_id:03d}_input.parquet")

    if not chunk_file.exists():
        print(f"Chunk {chunk_id} input file not found")
        return None

    table = pq.read_table(chunk_file)
    total_rows = len(table)

    # Sample molecules
    sample_indices = np.random.choice(total_rows, min(sample_size, total_rows), replace=False)
    smiles_list = [table['smiles'][i].as_py() for i in sample_indices]

    metrics = {
        'chunk_id': chunk_id,
        'total_rows': total_rows,
        'phenolic_oh_counts': [],
        'mol_weights': [],
        'num_aromatic_rings': [],
        'num_rotatable_bonds': [],
        'complexity_scores': []
    }

    for smi in smiles_list:
        mol = Chem.MolFromSmiles(smi)
        if mol is None:
            continue

        # Count phenolic OH groups
        pattern = Chem.MolFromSmarts('[OH][c]')
        phenolic_oh = len(mol.GetSubstructMatches(pattern)) if pattern else 0

        # Molecular descriptors
        mw = Descriptors.MolWt(mol)
        aromatic_rings = Descriptors.NumAromaticRings(mol)
        rotatable_bonds = Descriptors.NumRotatableBonds(mol)

        # Complexity score (custom metric)
        # Assume each phenolic OH can generate multiple products (per_site mode)
        # Complexity ~ phenolic_oh^k where k depends on transformation
        complexity = phenolic_oh ** 2  # Rough approximation

        metrics['phenolic_oh_counts'].append(phenolic_oh)
        metrics['mol_weights'].append(mw)
        metrics['num_aromatic_rings'].append(aromatic_rings)
        metrics['num_rotatable_bonds'].append(rotatable_bonds)
        metrics['complexity_scores'].append(complexity)

    # Statistics
    results = {
        'chunk_id': chunk_id,
        'total_rows': total_rows,
        'sample_size': len(metrics['phenolic_oh_counts']),
        'phenolic_oh': {
            'mean': np.mean(metrics['phenolic_oh_counts']),
            'median': np.median(metrics['phenolic_oh_counts']),
            'std': np.std(metrics['phenolic_oh_counts']),
            'max': np.max(metrics['phenolic_oh_counts']),
            'min': np.min(metrics['phenolic_oh_counts'])
        },
        'mol_weight': {
            'mean': np.mean(metrics['mol_weights']),
            'std': np.std(metrics['mol_weights'])
        },
        'complexity_score': {
            'mean': np.mean(metrics['complexity_scores']),
            'median': np.median(metrics['complexity_scores']),
            'max': np.max(metrics['complexity_scores'])
        },
        'predicted_products_per_mol': np.mean(metrics['complexity_scores']) * 3  # Rough estimate
    }

    return results

def main():
    print("="*80)
    print("CHUNK COMPLEXITY ANALYSIS")
    print("="*80)

    # Analyze chunks 0-2 (and optionally more)
    chunks_to_analyze = [0, 1, 2, 3, 4]  # First 5 chunks

    all_results = []
    for chunk_id in chunks_to_analyze:
        print(f"\nAnalyzing Chunk {chunk_id}...")
        result = analyze_chunk_complexity(chunk_id)
        if result:
            all_results.append(result)

            print(f"  Total rows: {result['total_rows']:,}")
            print(f"  Phenolic OH (mean): {result['phenolic_oh']['mean']:.2f}")
            print(f"  Phenolic OH (max): {result['phenolic_oh']['max']}")
            print(f"  Complexity score (mean): {result['complexity_score']['mean']:.1f}")
            print(f"  Predicted products/mol: {result['predicted_products_per_mol']:.1f}")

    # Comparison
    print("\n" + "="*80)
    print("COMPARATIVE ANALYSIS")
    print("="*80)

    if len(all_results) >= 2:
        baseline = all_results[0]
        for result in all_results[1:]:
            complexity_ratio = result['complexity_score']['mean'] / baseline['complexity_score']['mean']
            print(f"\nChunk {result['chunk_id']} vs Chunk 0:")
            print(f"  Complexity ratio: {complexity_ratio:.2f}x")
            print(f"  Phenolic OH ratio: {result['phenolic_oh']['mean'] / baseline['phenolic_oh']['mean']:.2f}x")

            if complexity_ratio > 2.0:
                print(f"  âš ï¸ WARNING: Chunk {result['chunk_id']} is {complexity_ratio:.1f}x more complex!")
                print(f"     Expected processing time: ~{44 * complexity_ratio:.0f} minutes")

    # Save results
    import json
    with open('chunk_complexity_analysis.json', 'w') as f:
        json.dump(all_results, f, indent=2)

    print(f"\nâœ“ Results saved to: chunk_complexity_analysis.json")

if __name__ == '__main__':
    main()
```

**è¿è¡Œ:**
```bash
python scripts/diagnose_chunk_complexity.py
```

#### A2. å†…å­˜è¶‹åŠ¿åˆ†æ

**æ–‡ä»¶:** `scripts/analyze_memory_trend.py`

```python
#!/usr/bin/env python3
"""
Analyze memory usage trends from transform logs.
"""

import re
import sys
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def analyze_memory_trend(log_file):
    """Extract and analyze memory trends from log."""

    log_path = Path(log_file)
    if not log_path.exists():
        print(f"Log file not found: {log_file}")
        return

    # Extract memory values
    memory_values = []
    flush_numbers = []

    with open(log_path, 'r', encoding='utf-8', errors='replace') as f:
        for line in f:
            # Match: "Pre-flush memory: XX.X%"
            match = re.search(r'Pre-flush memory: ([0-9.]+)%', line)
            if match:
                memory_values.append(float(match.group(1)))

            # Match: "Flush #XXX:"
            flush_match = re.search(r'Flush #(\d+):', line)
            if flush_match:
                flush_numbers.append(int(flush_match.group(1)))

    if not memory_values:
        print("No memory data found in log")
        return

    memory_array = np.array(memory_values)

    # Statistics
    print("="*80)
    print("MEMORY TREND ANALYSIS")
    print("="*80)
    print(f"Log file: {log_file}")
    print(f"Total samples: {len(memory_array)}")
    print(f"\nMemory Statistics:")
    print(f"  Start: {memory_array[0]:.1f}%")
    print(f"  End: {memory_array[-1]:.1f}%")
    print(f"  Mean: {np.mean(memory_array):.1f}%")
    print(f"  Std: {np.std(memory_array):.2f}%")
    print(f"  Min: {np.min(memory_array):.1f}%")
    print(f"  Max: {np.max(memory_array):.1f}%")
    print(f"  Drift: {memory_array[-1] - memory_array[0]:+.1f}%")

    # Check for memory leak
    if len(memory_array) > 100:
        # Linear regression to detect trend
        x = np.arange(len(memory_array))
        slope, intercept = np.polyfit(x, memory_array, 1)

        print(f"\nTrend Analysis:")
        print(f"  Slope: {slope:.6f}% per flush")
        print(f"  Projected drift (1000 flushes): {slope * 1000:.1f}%")

        if abs(slope) > 0.001:
            print(f"  âš ï¸ WARNING: Detected memory trend!")
            if slope > 0:
                print(f"     Memory is gradually increasing (possible leak)")
            else:
                print(f"     Memory is gradually decreasing (unusual)")

    # Plot
    plt.figure(figsize=(12, 6))
    plt.plot(memory_array, alpha=0.7, linewidth=0.5)
    plt.axhline(y=70, color='r', linestyle='--', label='70% limit')
    plt.xlabel('Flush Number')
    plt.ylabel('Memory %')
    plt.title(f'Memory Trend: {log_path.name}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    output_file = log_path.parent / f"{log_path.stem}_memory_trend.png"
    plt.savefig(output_file, dpi=150)
    print(f"\nâœ“ Plot saved to: {output_file}")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python analyze_memory_trend.py <log_file>")
        print("\nExample:")
        print("  python analyze_memory_trend.py data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_002_output/transform.log")
        sys.exit(1)

    analyze_memory_trend(sys.argv[1])
```

**è¿è¡Œ:**
```bash
python scripts/analyze_memory_trend.py data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_002_output/transform.log
```

#### A3. åŸºäºåˆ†æç»“æœçš„è§£å†³æ–¹æ¡ˆ

**åœºæ™¯1: å¦‚æœæ˜¯åˆ†å­å¤æ‚åº¦é—®é¢˜**

è§£å†³æ–¹æ¡ˆï¼šåŠ¨æ€chunkå¤§å°
```python
# ä¿®æ”¹ batch_transform_pipeline.py
# åœ¨ create_chunks() ä¸­æ·»åŠ å¤æ‚åº¦æ„ŸçŸ¥åˆ†å‰²

def create_chunks_adaptive(self):
    """Create chunks with adaptive sizing based on complexity."""

    # First pass: analyze complexity
    complexity_scores = self.estimate_complexity_by_rows(
        self.input_file,
        sample_rate=0.01  # Sample 1%
    )

    # Second pass: create variable-sized chunks
    chunks = []
    current_start = 0
    chunk_id = 0

    target_complexity_per_chunk = np.median(complexity_scores) * 1_000_000

    while current_start < total_rows:
        # Estimate how many rows to include based on complexity
        estimated_rows = self.estimate_rows_for_target_complexity(
            complexity_scores[current_start:],
            target_complexity_per_chunk
        )

        chunk_end = min(current_start + estimated_rows, total_rows)

        # Create chunk...
        chunk_id += 1
        current_start = chunk_end
```

**åœºæ™¯2: å¦‚æœæ˜¯å†…å­˜é—®é¢˜**

è§£å†³æ–¹æ¡ˆï¼šå®šæœŸé‡å¯workers
```python
# åœ¨ 08_transform_library_v2.py ä¸­æ·»åŠ 
# æ¯å¤„ç†Nä¸ªbatchåé‡å¯worker pool

if batch_idx % 10 == 0 and batch_idx > 0:
    logger.info("Restarting worker pool to clear memory...")
    executor.shutdown(wait=True)
    executor = ProcessPoolExecutor(max_workers=workers)
```

**åœºæ™¯3: å¦‚æœæ˜¯Windowsç‰¹å®šé—®é¢˜**

ç«‹å³è¿ç§»åˆ°Linuxï¼ˆè§ä»»åŠ¡Dï¼‰

#### A4. é‡æ–°è¯„ä¼°Timeoutç­–ç•¥

**ä¸è¦ç®€å•å¢åŠ timeoutï¼Œè€Œæ˜¯:**

1. **è®¾ç½®åˆç†timeoutåŸºå‡†**
```python
# åŸºäºå¤æ‚åº¦åŠ¨æ€è®¡ç®—timeout
base_timeout = 3600  # 1 hour baseline
complexity_factor = chunk_complexity_score / baseline_complexity
timeout = base_timeout * complexity_factor * 1.5  # 1.5x safety margin
```

2. **æ·»åŠ è¿›åº¦ç›‘æ§**
```python
# æ£€æµ‹å¡æ­» vs æ­£å¸¸ç¼“æ…¢å¤„ç†
# å¦‚æœé•¿æ—¶é—´æ— flushè¾“å‡º â†’ çœŸçš„å¡æ­»ï¼Œåº”è¯¥kill
# å¦‚æœæŒç»­æœ‰flush â†’ æ­£å¸¸ä½†æ…¢ï¼Œåº”è¯¥ç»§ç»­ç­‰å¾…
```

---

### ä»»åŠ¡Bï¼šRepositoryæ•´ç†ä¸æ¸…ç† ğŸ—‚ï¸ **é«˜ä¼˜å…ˆçº§**

**ç›®æ ‡:**
1. è§„èŒƒç›®å½•ç»“æ„
2. åˆ é™¤ä¸´æ—¶/æµ‹è¯•æ–‡ä»¶
3. ä¿ç•™é‡è¦è¾“å‡ºå’Œæ–‡æ¡£
4. å‡†å¤‡Gitæäº¤

**å½“å‰ç›®å½•æ··ä¹±çŠ¶æ€:**

```
E:/Projects/halogenator/
â”œâ”€â”€ *.md                    â† æ•£è½çš„æ–‡æ¡£ï¼ˆ30+ä¸ªï¼‰
â”œâ”€â”€ *.py                    â† æµ‹è¯•è„šæœ¬ï¼ˆ20+ä¸ªï¼‰
â”œâ”€â”€ *.log                   â† æ—¥å¿—æ–‡ä»¶ï¼ˆ10+ä¸ªï¼‰
â”œâ”€â”€ *.txt                   â† ä¸´æ—¶æ–‡ä»¶
â”œâ”€â”€ configs/                â† é…ç½®æ–‡ä»¶ï¼ˆéƒ¨åˆ†ä¸´æ—¶ï¼‰
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ transforms/
â”‚   â”‚   â”‚   â”œâ”€â”€ OPT_*       â† Grid searchæµ‹è¯•è¾“å‡ºï¼ˆéœ€åˆ é™¤ï¼‰
â”‚   â”‚   â”‚   â”œâ”€â”€ TEST_*      â† å„ç§æµ‹è¯•è¾“å‡ºï¼ˆéœ€åˆ é™¤ï¼‰
â”‚   â”‚   â”‚   â””â”€â”€ VALIDATION_* â† éªŒè¯æµ‹è¯•ï¼ˆéœ€åˆ é™¤ï¼‰
â”‚   â”‚   â””â”€â”€ nplike_v2/      â† æ­£å¼æ•°æ®ï¼ˆä¿ç•™ï¼‰
â”‚   â”œâ”€â”€ test/               â† æµ‹è¯•æ•°æ®ï¼ˆéœ€åˆ é™¤ï¼‰
â”‚   â”œâ”€â”€ viz/                â† å¯è§†åŒ–ä¸´æ—¶è¾“å‡ºï¼ˆéœ€åˆ é™¤ï¼‰
â”‚   â””â”€â”€ viz_v2/             â† å¯è§†åŒ–ä¸´æ—¶è¾“å‡ºï¼ˆéœ€åˆ é™¤ï¼‰
â”œâ”€â”€ scripts/                â† è„šæœ¬æ··ä¹±
â””â”€â”€ src/                    â† æºä»£ç ï¼ˆä¿ç•™ï¼‰
```

#### B1. åˆ›å»ºç›®æ ‡ç›®å½•ç»“æ„

**æœŸæœ›ç»“æ„:**
```
E:/Projects/halogenator/
â”œâ”€â”€ docs/                   â† æ‰€æœ‰.mdæ–‡æ¡£
â”‚   â”œâ”€â”€ archive/            â† æ—§ç‰ˆæœ¬æ–‡æ¡£
â”‚   â”œâ”€â”€ guides/             â† ä½¿ç”¨æŒ‡å—
â”‚   â””â”€â”€ reports/            â† ä¼šè¯æŠ¥å‘Š
â”œâ”€â”€ logs/                   â† å½’æ¡£æ—¥å¿—
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ production/         â† ç”Ÿäº§è„šæœ¬
â”‚   â”œâ”€â”€ diagnosis/          â† è¯Šæ–­å·¥å…·
â”‚   â””â”€â”€ archive/            â† ä¸´æ—¶è„šæœ¬å½’æ¡£
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ production/         â† ç”Ÿäº§é…ç½®
â”‚   â””â”€â”€ archive/            â† æµ‹è¯•é…ç½®
â”œâ”€â”€ data/
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ transforms/
â”‚       â”‚   â””â”€â”€ polyphenol-2X_BATCHED/  â† å”¯ä¸€ä¿ç•™çš„æ­£å¼è¾“å‡º
â”‚       â””â”€â”€ nplike_v2/      â† ä¿ç•™
â”œâ”€â”€ src/                    â† æºä»£ç 
â””â”€â”€ tests/                  â† æµ‹è¯•ä»£ç 
```

#### B2. æ•´ç†è„šæœ¬

**æ–‡ä»¶:** `scripts/cleanup_repository.py`

```python
#!/usr/bin/env python3
"""
Repository cleanup and reorganization script.
"""

import shutil
from pathlib import Path
import json

class RepositoryCleanup:
    def __init__(self, repo_root):
        self.repo_root = Path(repo_root)
        self.dry_run = True  # Safety first
        self.actions = []

    def analyze(self):
        """Analyze current state and plan actions."""

        print("="*80)
        print("REPOSITORY CLEANUP ANALYSIS")
        print("="*80)

        # Find all .md files
        md_files = list(self.repo_root.glob("*.md"))
        print(f"\nğŸ“„ Markdown files in root: {len(md_files)}")
        for f in md_files:
            self.actions.append({
                'type': 'move',
                'source': str(f),
                'dest': f"docs/{f.name}",
                'reason': 'Documentation to docs/'
            })

        # Find test directories/files to delete
        test_patterns = [
            "data/output/transforms/OPT_*",
            "data/output/transforms/TEST_*",
            "data/output/transforms/VALIDATION_*",
            "data/test/",
            "data/viz/",
            "data/viz_v2/",
            "data/viz_base_libs/",
            "tmp/"
        ]

        for pattern in test_patterns:
            matches = list(self.repo_root.glob(pattern))
            for match in matches:
                if match.exists():
                    size = self.get_size(match)
                    self.actions.append({
                        'type': 'delete',
                        'path': str(match),
                        'size': size,
                        'reason': 'Test/temporary data'
                    })

        # Find .log files
        log_files = list(self.repo_root.glob("*.log"))
        print(f"\nğŸ“ Log files in root: {len(log_files)}")
        for f in log_files:
            self.actions.append({
                'type': 'move',
                'source': str(f),
                'dest': f"logs/{f.name}",
                'reason': 'Log to logs/'
            })

        # Find standalone .py scripts
        py_files = [
            f for f in self.repo_root.glob("*.py")
            if f.name not in ['setup.py', '__init__.py']
        ]
        print(f"\nğŸ Python scripts in root: {len(py_files)}")
        for f in py_files:
            # Categorize
            if 'test' in f.name.lower() or 'validate' in f.name.lower():
                dest = f"scripts/archive/{f.name}"
            elif 'diagnose' in f.name.lower() or 'analyze' in f.name.lower():
                dest = f"scripts/diagnosis/{f.name}"
            elif 'optimize' in f.name.lower() or 'batch' in f.name.lower():
                dest = f"scripts/production/{f.name}"
            else:
                dest = f"scripts/archive/{f.name}"

            self.actions.append({
                'type': 'move',
                'source': str(f),
                'dest': dest,
                'reason': 'Script organization'
            })

        # Summary
        print(f"\n" + "="*80)
        print(f"PLANNED ACTIONS SUMMARY")
        print(f"="*80)

        action_counts = {}
        for action in self.actions:
            action_type = action['type']
            action_counts[action_type] = action_counts.get(action_type, 0) + 1

        for action_type, count in action_counts.items():
            print(f"  {action_type.upper()}: {count} items")

        # Calculate space to be freed
        delete_size = sum(
            action.get('size', 0)
            for action in self.actions
            if action['type'] == 'delete'
        )
        print(f"\nğŸ’¾ Space to be freed: {delete_size / (1024**3):.2f} GB")

        return self.actions

    def get_size(self, path):
        """Get size of file or directory in bytes."""
        path = Path(path)
        if path.is_file():
            return path.stat().st_size
        elif path.is_dir():
            return sum(f.stat().st_size for f in path.rglob('*') if f.is_file())
        return 0

    def execute(self, dry_run=True):
        """Execute planned actions."""

        self.dry_run = dry_run

        mode_str = "DRY RUN" if dry_run else "EXECUTING"
        print(f"\n{'='*80}")
        print(f"{mode_str} - Repository Cleanup")
        print(f"{'='*80}")

        success_count = 0
        error_count = 0

        for i, action in enumerate(self.actions, 1):
            print(f"\n[{i}/{len(self.actions)}] {action['type'].upper()}: {action.get('source', action.get('path'))}")

            try:
                if action['type'] == 'move':
                    if not dry_run:
                        source = Path(action['source'])
                        dest = self.repo_root / action['dest']
                        dest.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(source), str(dest))
                    print(f"  â†’ {action['dest']}")
                    success_count += 1

                elif action['type'] == 'delete':
                    path = Path(action['path'])
                    size_mb = action.get('size', 0) / (1024**2)
                    if not dry_run:
                        if path.is_dir():
                            shutil.rmtree(path)
                        else:
                            path.unlink()
                    print(f"  âœ— Deleted ({size_mb:.1f} MB)")
                    success_count += 1

            except Exception as e:
                print(f"  âœ— ERROR: {e}")
                error_count += 1

        # Summary
        print(f"\n{'='*80}")
        print(f"CLEANUP {'DRY RUN' if dry_run else 'EXECUTION'} COMPLETE")
        print(f"{'='*80}")
        print(f"  Success: {success_count}/{len(self.actions)}")
        print(f"  Errors: {error_count}")

        if dry_run:
            print(f"\nâš ï¸  This was a DRY RUN. No files were actually moved or deleted.")
            print(f"   Review the actions above. To execute for real, run:")
            print(f"   python scripts/cleanup_repository.py --execute")
        else:
            print(f"\nâœ“ Repository cleanup complete!")

            # Save cleanup report
            report_file = self.repo_root / "docs" / "reports" / "cleanup_report.json"
            report_file.parent.mkdir(parents=True, exist_ok=True)
            with open(report_file, 'w') as f:
                json.dump({
                    'actions': self.actions,
                    'success_count': success_count,
                    'error_count': error_count
                }, f, indent=2)
            print(f"   Report saved to: {report_file}")

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Clean up and reorganize repository')
    parser.add_argument('--execute', action='store_true', help='Execute cleanup (default is dry run)')
    parser.add_argument('--repo', default='.', help='Repository root path')
    args = parser.parse_args()

    cleanup = RepositoryCleanup(args.repo)
    cleanup.analyze()
    cleanup.execute(dry_run=not args.execute)

if __name__ == '__main__':
    main()
```

**æ‰§è¡Œæ­¥éª¤:**

1. **Dry runåˆ†æ:**
```bash
cd E:/Projects/halogenator
python scripts/cleanup_repository.py
# ä»”ç»†æ£€æŸ¥è¾“å‡ºï¼Œç¡®è®¤è¦åˆ é™¤/ç§»åŠ¨çš„æ–‡ä»¶
```

2. **å®¡æŸ¥åˆ é™¤åˆ—è¡¨:**
```bash
# ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹ä¿ç•™é¡¹ï¼š
# - data/output/nplike_v2/ (åŸå§‹å¤„ç†æ•°æ®)
# - data/output/transforms/polyphenol-2X_BATCHED/ (å½“å‰pipelineè¾“å‡º)
# - src/ (æºä»£ç )
# - configs/halogen_rules_by_class.yaml (ç”Ÿäº§é…ç½®)
# - configs/transforms.yaml (ç”Ÿäº§é…ç½®)
```

3. **ç¡®è®¤åæ‰§è¡Œ:**
```bash
python scripts/cleanup_repository.py --execute
```

#### B3. æ–‡æ¡£åˆ†ç±»æ•´ç†

**æ‰‹åŠ¨æ•´ç†å»ºè®®:**

```bash
mkdir -p docs/{archive,guides,reports}

# ä¼šè¯æŠ¥å‘Š
mv SESSION_* docs/reports/
mv IMPLEMENTATION_* docs/reports/
mv WORK_REPORT_* docs/reports/

# æŠ€æœ¯æ–‡æ¡£
mv ROOT_CAUSE_ANALYSIS_* docs/reports/
mv SOLUTION_VALIDATED_* docs/reports/

# ä½¿ç”¨æŒ‡å—
mv USAGE_GUIDE.md docs/guides/
mv SUGAR_FILTER_GUIDE.md docs/guides/
mv TEST_OBSERVATION_GUIDE.md docs/guides/

# å…¶ä»–å½’æ¡£
mv *.md docs/archive/  # å‰©ä½™æ–‡æ¡£
```

#### B4. é…ç½®æ–‡ä»¶æ•´ç†

```bash
cd configs/

# ä¿ç•™ç”Ÿäº§é…ç½®
mkdir -p production archive

# ç”Ÿäº§é…ç½®ï¼ˆä¸ç§»åŠ¨ï¼‰
# - halogen_rules_by_class.yaml
# - transforms.yaml

# å½’æ¡£æµ‹è¯•é…ç½®
mv *_k2.yaml archive/
mv *_k3.yaml archive/
mv test_*.yaml archive/
mv macro_verify_*.yaml archive/
```

---

### ä»»åŠ¡Cï¼šGitæäº¤ä¸Githubæ¨é€ ğŸ“¤ **ä¸­ä¼˜å…ˆçº§**

**ç›®æ ‡:**
1. æäº¤æ‰€æœ‰æœ‰ä»·å€¼çš„ä»£ç å’Œæ–‡æ¡£
2. æ¨é€åˆ°Githubè¿œç¨‹ä»“åº“
3. å‡†å¤‡åœ¨Linuxä¸Šclone

#### C1. å‡†å¤‡.gitignore

**æ£€æŸ¥/æ›´æ–° `.gitignore`:**

```bash
# æŸ¥çœ‹å½“å‰.gitignore
cat .gitignore

# åº”è¯¥åŒ…å«ï¼š
# Data files
data/output/
*.parquet
*.pkl

# Logs
*.log
logs/

# Temporary files
tmp/
temp/
__pycache__/
*.pyc
*.pyo

# IDE
.vscode/
.idea/

# OS
.DS_Store
Thumbs.db

# Large files
*.gz
*.zip
*.tar

# Test outputs
*_TEST_*
*_OPT_*
optimization_results_*.csv
```

#### C2. Gitæäº¤æµç¨‹

**æ­¥éª¤:**

```bash
cd E:/Projects/halogenator

# 1. æŸ¥çœ‹å½“å‰åˆ†æ”¯å’ŒçŠ¶æ€
git status
git branch

# 2. å¦‚æœåœ¨fixåˆ†æ”¯ï¼Œè€ƒè™‘åˆå¹¶æˆ–åˆ›å»ºæ–°åˆ†æ”¯
# å½“å‰åˆ†æ”¯ï¼šfix/pr2-contract-and-sugar-accept
# å»ºè®®åˆ›å»ºæ–°åˆ†æ”¯ç”¨äºbatch pipelineå·¥ä½œ

git checkout -b feature/batch-transform-pipeline

# 3. Stageæ•´ç†åçš„æ–‡ä»¶
git add docs/
git add scripts/production/
git add scripts/diagnosis/
git add batch_transform_pipeline.py
git add optimize_parameters.py
git add .gitignore

# 4. æŸ¥çœ‹å°†è¦æäº¤çš„å†…å®¹
git status
git diff --staged

# 5. åˆ›å»ºè¯¦ç»†çš„commit message
cat > commit_message.txt << 'EOF'
feat: Batch transform pipeline with parameter optimization

Major changes:
1. Parameter optimization grid search framework
   - Created optimize_parameters.py for testing worker/batch configs
   - Tested 4 configurations on 150K subset
   - Identified safe config: workers=12, max-in-flight=8

2. Batch processing pipeline implementation
   - Created batch_transform_pipeline.py
   - Automatic chunking of large datasets (1M rows/chunk)
   - Checkpoint and resume capability
   - State tracking with pipeline_state.json
   - Automatic result merging

3. Repository reorganization
   - Moved documentation to docs/
   - Organized scripts into production/diagnosis/archive
   - Cleaned up test outputs and temporary files
   - Improved .gitignore

4. Diagnostic tools
   - diagnose_chunk_complexity.py for complexity analysis
   - analyze_memory_trend.py for memory profiling
   - cleanup_repository.py for automated cleanup

Technical details:
- Workers=16, max-in-flight=6, batch=50K configuration
- 4-hour timeout per chunk (needs investigation)
- Successfully processed 2/14 chunks of polyphenol-2X
- Discovered chunk complexity variance issue

Known issues:
- Timeout issues on complex chunks (needs deeper investigation)
- Chunk 2 timeout after 4 hours (7.3M products generated)
- Possible Windows ProcessPoolExecutor limitations

Next steps:
- Investigate chunk complexity distribution
- Test on Linux for comparison
- Optimize timeout strategy based on complexity

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
EOF

git commit -F commit_message.txt

# 6. æ¨é€åˆ°Github
# å¦‚æœæ˜¯æ–°åˆ†æ”¯ï¼Œéœ€è¦è®¾ç½®upstream
git push -u origin feature/batch-transform-pipeline

# 7. åˆ›å»ºPull Request (å¯é€‰)
# å¦‚æœéœ€è¦code reviewï¼Œåœ¨Githubä¸Šåˆ›å»ºPR
```

#### C3. éªŒè¯è¿œç¨‹æ¨é€

```bash
# æ£€æŸ¥è¿œç¨‹ä»“åº“
git remote -v

# å¦‚æœæ²¡æœ‰é…ç½®ï¼Œæ·»åŠ Github remote
# git remote add origin https://github.com/YOUR_USERNAME/halogenator.git

# æ¨é€åéªŒè¯
git log --oneline -5
git branch -r
```

#### C4. æ ‡ç­¾é‡è¦ç‰ˆæœ¬

```bash
# ä¸ºbatch pipelineå®ç°æ‰“æ ‡ç­¾
git tag -a v0.2.0-batch-pipeline -m "Batch processing pipeline implementation"
git push origin v0.2.0-batch-pipeline
```

---

### ä»»åŠ¡Dï¼šLinuxè¿ç§»ä¸å…¼å®¹æ€§éªŒè¯ ğŸ§ **é«˜ä¼˜å…ˆçº§**

**ç›®æ ‡:**
1. åœ¨LinuxæœåŠ¡å™¨ä¸ŠæˆåŠŸè¿è¡Œ
2. éªŒè¯æ€§èƒ½å·®å¼‚
3. å¯¹æ¯”Windowsçš„ç¨³å®šæ€§é—®é¢˜

#### D1. ç¯å¢ƒå‡†å¤‡

**LinuxæœåŠ¡å™¨è¦æ±‚:**
```
OS: Ubuntu 20.04+ / CentOS 7+
Python: 3.8+
RAM: 32GB+
Storage: 500GB+ (SSDæ¨è)
CPU: 16+ cores
```

**å…‹éš†ä»“åº“:**

```bash
# SSHç™»å½•LinuxæœåŠ¡å™¨
ssh user@linux-server

# å…‹éš†ä»“åº“
cd /home/user/projects/  # æˆ–å…¶ä»–å·¥ä½œç›®å½•
git clone https://github.com/YOUR_USERNAME/halogenator.git
cd halogenator

# åˆ‡æ¢åˆ°batch pipelineåˆ†æ”¯
git checkout feature/batch-transform-pipeline
```

#### D2. ä¾èµ–å®‰è£…

**åˆ›å»ºcondaç¯å¢ƒ:**

```bash
# å¦‚æœæ²¡æœ‰condaï¼Œå…ˆå®‰è£…Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh

# åˆ›å»ºç¯å¢ƒ
conda create -n halo-p0 python=3.9
conda activate halo-p0

# å®‰è£…ä¾èµ–
pip install rdkit-pypi
pip install pyarrow pandas numpy
pip install matplotlib  # for diagnostics
pip install tqdm  # optional, for progress bars

# éªŒè¯å®‰è£…
python -c "from rdkit import Chem; print('RDKit OK')"
python -c "import pyarrow; print('PyArrow OK')"
```

**æ£€æŸ¥requirements.txt:**

å¦‚æœæ²¡æœ‰ï¼Œåˆ›å»ºï¼š
```bash
cat > requirements.txt << EOF
rdkit-pypi>=2022.9.5
pyarrow>=10.0.0
pandas>=1.5.0
numpy>=1.23.0
pyyaml>=6.0
tqdm>=4.64.0
matplotlib>=3.6.0
EOF

pip install -r requirements.txt
```

#### D3. æ•°æ®ä¼ è¾“

**æ–¹æ¡ˆA: rsync (æ¨è)**

```bash
# ä»Windowsä¼ è¾“åˆ°Linux
# åœ¨Windowsä¸Š (ä½¿ç”¨WSLæˆ–Git Bash)
rsync -avz --progress \
  /e/Projects/halogenator/data/output/nplike_v2/ \
  user@linux-server:/home/user/projects/halogenator/data/output/nplike_v2/

# æˆ–åªä¼ è¾“polyphenol-2X
rsync -avz --progress \
  /e/Projects/halogenator/data/output/nplike_v2/polyphenol-2X/ \
  user@linux-server:/home/user/projects/halogenator/data/output/nplike_v2/polyphenol-2X/
```

**æ–¹æ¡ˆB: ä½¿ç”¨scp**

```bash
# å‹ç¼©åä¼ è¾“
cd /e/Projects/halogenator/data/output/nplike_v2/
tar -czf polyphenol-2X.tar.gz polyphenol-2X/

scp polyphenol-2X.tar.gz user@linux-server:/home/user/projects/halogenator/data/

# åœ¨Linuxä¸Šè§£å‹
ssh user@linux-server
cd /home/user/projects/halogenator/data/output/nplike_v2/
tar -xzf ../../polyphenol-2X.tar.gz
```

#### D4. å…¼å®¹æ€§éªŒè¯æµ‹è¯•

**Test 1: åŸºç¡€åŠŸèƒ½æµ‹è¯•**

```bash
cd /home/user/projects/halogenator

# æµ‹è¯•transformè„šæœ¬å¯ä»¥è¿è¡Œ
python scripts/08_transform_library_v2.py --help

# æµ‹è¯•batch pipeline
python batch_transform_pipeline.py --help
```

**Test 2: å°è§„æ¨¡éªŒè¯ï¼ˆ1å°æ—¶ï¼‰**

```bash
# åˆ›å»ºæµ‹è¯•chunk (100K rows)
python -c "
import pyarrow.parquet as pq
table = pq.read_table('data/output/nplike_v2/polyphenol-2X/products.parquet')
subset = table.slice(0, 100000)
pq.write_table(subset, 'data/test_100k.parquet')
print('Created test_100k.parquet')
"

# è¿è¡Œå•æ¬¡transformæµ‹è¯•
python scripts/08_transform_library_v2.py apply \
  --input data/test_100k.parquet \
  --outdir data/output/test_linux_100k \
  --xf-config configs/transforms.yaml \
  --xf-name FG_PHENOL_OH__OH__TO__OMe \
  --workers 16 \
  --max-in-flight 6 \
  --batch-size 50000 \
  --use-bloom-filter

# æ£€æŸ¥ç»“æœ
ls -lh data/output/test_linux_100k/
cat data/output/test_linux_100k/SUMMARY.json
```

**Test 3: Chunk 2 å¯¹æ¯”æµ‹è¯•ï¼ˆå…³é”®ï¼ï¼‰**

```bash
# ä¼ è¾“chunk 2è¾“å…¥æ–‡ä»¶
rsync -avz \
  /e/Projects/halogenator/data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_002_input.parquet \
  user@linux-server:/home/user/projects/halogenator/data/chunk_002_input.parquet

# åœ¨Linuxä¸Šè¿è¡Œchunk 2æµ‹è¯•
# ä½¿ç”¨ç›¸åŒé…ç½®ï¼Œç›‘æ§æ˜¯å¦ä¹Ÿä¼štimeout
time python scripts/08_transform_library_v2.py apply \
  --input data/chunk_002_input.parquet \
  --outdir data/output/test_linux_chunk2 \
  --xf-config configs/transforms.yaml \
  --xf-name FG_PHENOL_OH__OH__TO__OMe \
  --workers 16 \
  --max-in-flight 6 \
  --batch-size 50000 \
  --use-bloom-filter \
  --bloom-expected-items 10000000 \
  2>&1 | tee linux_chunk2_test.log

# å…³é”®å¯¹æ¯”ç‚¹ï¼š
# 1. æ˜¯å¦åœ¨4å°æ—¶å†…å®Œæˆï¼Ÿ
# 2. å¦‚æœå®Œæˆï¼Œç”¨æ—¶å¤šå°‘ï¼Ÿ
# 3. å†…å­˜ä½¿ç”¨æ˜¯å¦æ›´ç¨³å®šï¼Ÿ
# 4. äº§å“æ•°é‡æ˜¯å¦ä¸€è‡´ï¼Ÿ

# å¦‚æœLinuxèƒ½åœ¨2-3å°æ—¶å®Œæˆchunk 2ï¼Œè¯´æ˜æ˜¯Windowsç‰¹å®šé—®é¢˜ï¼
```

#### D5. æ€§èƒ½å¯¹æ¯”åˆ†æ

**åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š:**

```python
# scripts/compare_windows_linux.py
"""
Compare Windows vs Linux performance for same workload.
"""

import json
from pathlib import Path

def compare_results():
    # Windows chunk 0 results
    windows_c0 = {
        'platform': 'Windows',
        'chunk': 0,
        'time': 2652,  # seconds
        'throughput': 1476.7
    }

    # Linux test results (to be filled)
    linux_test = {
        'platform': 'Linux',
        'chunk': 'test_100k',
        'time': None,  # From actual test
        'throughput': None
    }

    # Analysis...
    print("Platform Performance Comparison")
    print("="*80)
    # ...
```

#### D6. ç”Ÿäº§éƒ¨ç½²å†³ç­–

**åŸºäºLinuxæµ‹è¯•ç»“æœå†³ç­–:**

**åœºæ™¯A: Linuxæ˜¾è‘—æ›´å¿«/ç¨³å®š**
â†’ ç«‹å³è¿ç§»æ‰€æœ‰ä»»åŠ¡åˆ°Linux
â†’ åœ¨Linuxä¸Šè¿è¡Œå®Œæ•´batch pipeline

**åœºæ™¯B: Linuxä¸Windowsç›¸ä¼¼**
â†’ é—®é¢˜ä¸æ˜¯å¹³å°ç‰¹å®š
â†’ éœ€è¦æ›´æ·±å…¥è°ƒæŸ¥chunk complexity
â†’ å¯ä»¥åœ¨ä»»ä¸€å¹³å°ç»§ç»­

**åœºæ™¯C: Linuxæ›´æ…¢**
â†’ æ£€æŸ¥Linuxç¯å¢ƒé…ç½®ï¼ˆCPU governor, NUMAç­‰ï¼‰
â†’ ä¼˜åŒ–Linuxç³»ç»Ÿå‚æ•°

---

## ç¬¬å››éƒ¨åˆ†ï¼šå…³é”®æ–‡ä»¶ç´¢å¼•

### ç”Ÿäº§ä»£ç 

```
src/halogenator/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ cli.py                  # CLIå…¥å£
â”œâ”€â”€ enumerate_k.py          # Kæ¬¡æšä¸¾æ ¸å¿ƒ
â”œâ”€â”€ transform_engine.py     # Transformå¼•æ“
â”œâ”€â”€ schema.py              # æ•°æ®schema
â””â”€â”€ sugar_mask.py          # Sugarè¿‡æ»¤

scripts/
â”œâ”€â”€ production/
â”‚   â”œâ”€â”€ 08_transform_library_v2.py  # Transformä¸»è„šæœ¬
â”‚   â”œâ”€â”€ batch_transform_pipeline.py # Batch pipeline
â”‚   â””â”€â”€ optimize_parameters.py      # å‚æ•°ä¼˜åŒ–
â””â”€â”€ diagnosis/
    â”œâ”€â”€ diagnose_chunk_complexity.py  # å¾…åˆ›å»º
    â””â”€â”€ analyze_memory_trend.py       # å¾…åˆ›å»º
```

### é…ç½®æ–‡ä»¶

```
configs/
â”œâ”€â”€ halogen_rules_by_class.yaml  # NPåˆ†ç±»halogenè§„åˆ™
â””â”€â”€ transforms.yaml              # Transformå®šä¹‰
```

### å½“å‰è¾“å‡º

```
data/output/
â”œâ”€â”€ nplike_v2/
â”‚   â””â”€â”€ polyphenol-2X/
â”‚       â””â”€â”€ products.parquet     # 13.79M rows, 504MB
â””â”€â”€ transforms/
    â””â”€â”€ polyphenol-2X_BATCHED/
        â”œâ”€â”€ pipeline_state.json  # PipelineçŠ¶æ€
        â””â”€â”€ chunks/
            â”œâ”€â”€ chunk_000_output/  # âœ… å®Œæˆ
            â”œâ”€â”€ chunk_001_output/  # âœ… å®Œæˆ
            â”œâ”€â”€ chunk_002_output/  # âŒ Timeout (éƒ¨åˆ†è¾“å‡º671MB)
            â””â”€â”€ chunk_003-013_*/   # â¸ï¸ å¾…å¤„ç†
```

### æ–‡æ¡£

```
docs/
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ SESSION_HANDOFF_BATCH_PIPELINE_AND_CLEANUP.md  # æœ¬æ–‡æ¡£
â”œâ”€â”€ guides/
â”‚   â””â”€â”€ USAGE_GUIDE.md
â””â”€â”€ archive/
    â””â”€â”€ *.md
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šæŠ€æœ¯ç»†èŠ‚ä¸æ³¨æ„äº‹é¡¹

### 5.1 å·²çŸ¥çš„ç³»ç»Ÿæ€§é—®é¢˜

#### é—®é¢˜1: ProcessPoolExecutoråœ¨Windowsä¸Šçš„ä¸ç¨³å®šæ€§

**è¡¨ç°:**
- 150Kæµ‹è¯•æˆåŠŸï¼Œ500K+å¤±è´¥
- Workerè¿›ç¨‹è¢«çªç„¶ç»ˆæ­¢ï¼ˆBrokenProcessPoolï¼‰
- é•¿æ—¶é—´è¿è¡Œåæ›´å®¹æ˜“å´©æºƒ

**åŸå› åˆ†æ:**
- Windows multiprocessingä½¿ç”¨spawnè€Œéfork
- Process serialization overhead
- å¯èƒ½çš„file handleæ³„æ¼

**ç¼“è§£æªæ–½:**
- ä½¿ç”¨è¾ƒå°çš„chunk size
- å®šæœŸé‡å¯worker pool
- è¿ç§»åˆ°Linux

#### é—®é¢˜2: Chunkå¤æ‚åº¦ä¸å‡åŒ€

**è¡¨ç°:**
- Chunk 0: 44åˆ†é’Ÿ
- Chunk 2: >4å°æ—¶ (5.4å€å·®å¼‚)
- äº§å“æ•°é‡: 2.47M vs 7.31M (3å€å·®å¼‚)

**å¯èƒ½åŸå› :**
- polyphenol-2Xå¯èƒ½æŒ‰åˆ†å­é‡æˆ–å¤æ‚åº¦æ’åº
- åç»­chunksåŒ…å«æ›´å¤šphenolic OHçš„åˆ†å­
- Per-site modeå¯¼è‡´æŒ‡æ•°çº§äº§å“å¢é•¿

**éœ€è¦éªŒè¯:**
- è¿è¡Œcomplexity analysisè„šæœ¬
- æ£€æŸ¥åŸå§‹æ•°æ®é›†çš„æ’åºæ–¹å¼

#### é—®é¢˜3: å†…å­˜ä½¿ç”¨çš„éçº¿æ€§å¢é•¿

**è§‚å¯Ÿ:**
- Chunk 0: æœªæ˜ç¡®è®°å½•å†…å­˜
- Chunk 1: å†…å­˜ç¨³å®š
- Chunk 2: 45.7%ç¨³å®šï¼ˆä½†å¤„ç†æ—¶é—´å¼‚å¸¸é•¿ï¼‰

**å¯èƒ½åŸå› :**
- ä¸æ˜¯å†…å­˜é—®é¢˜ï¼ˆ45.7%å¾ˆå®‰å…¨ï¼‰
- è€Œæ˜¯è®¡ç®—å¤æ‚åº¦é—®é¢˜
- æˆ–I/Oç“¶é¢ˆ

### 5.2 é…ç½®å‚æ•°çš„æƒè¡¡

**Workersæ•°é‡:**
- æ›´å¤šworkers â†’ æ›´å¿«ï¼Œä½†å†…å­˜å ç”¨é«˜
- Windowså»ºè®®ï¼š12-16
- Linuxå¯èƒ½å¯ä»¥æ›´é«˜ï¼š24-32

**Max-in-flight:**
- æ§åˆ¶å¹¶å‘batchæ•°é‡
- å¤ªé«˜ â†’ å†…å­˜å‹åŠ›ï¼Œqueue buildup
- å¤ªä½ â†’ workersç©ºé—²ï¼Œæ•ˆç‡ä½
- å½“å‰ï¼š6-8æ˜¯å¹³è¡¡ç‚¹

**Batch size:**
- 50Kæ˜¯æµ‹è¯•éªŒè¯çš„å€¼
- æ›´å¤§ â†’ å‡å°‘overheadï¼Œä½†å¢åŠ å•batchå¤„ç†æ—¶é—´
- æ›´å° â†’ æ›´çµæ´»ï¼Œä½†overheadå¢åŠ 

**Timeout:**
- å½“å‰4å°æ—¶ä¸å¤Ÿï¼ˆchunk 2è¯æ˜ï¼‰
- ä¸åº”ç®€å•å¢åŠ ï¼Œè€Œåº”åŸºäºå¤æ‚åº¦åŠ¨æ€è®¡ç®—
- æˆ–æ”¹è¿›chunkåˆ’åˆ†ç­–ç•¥

### 5.3 æ•°æ®å®Œæ•´æ€§æ£€æŸ¥

**Chunkè¾“å‡ºéªŒè¯:**

```bash
# æ£€æŸ¥æ‰€æœ‰å®Œæˆçš„chunks
for chunk_dir in data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_*_output/; do
    chunk_id=$(basename $chunk_dir | grep -oP '\d+')

    if [ -f "$chunk_dir/SUMMARY.json" ]; then
        products=$(jq '.unique_products' "$chunk_dir/SUMMARY.json")
        echo "Chunk $chunk_id: âœ“ $products products"
    else
        if [ -f "$chunk_dir/products.parquet" ]; then
            size=$(du -h "$chunk_dir/products.parquet" | cut -f1)
            echo "Chunk $chunk_id: âš  No SUMMARY, but $size products.parquet exists"
        else
            echo "Chunk $chunk_id: âœ— No output"
        fi
    fi
done
```

### 5.4 Resume Pipelineçš„æ­£ç¡®æ–¹æ³•

**å½“å‰çŠ¶æ€æ–‡ä»¶ç»“æ„:**

```json
{
  "chunks": [...],
  "completed_chunks": [0, 1],
  "failed_chunks": [2],
  "start_time": "...",
  "end_time": null
}
```

**Resumeæ–¹æ³•:**

```bash
# Pipelineä¼šè‡ªåŠ¨è·³è¿‡completed chunks
# ä½†éœ€è¦æ‰‹åŠ¨å¤„ç†failed chunks

# é€‰é¡¹1: æ¸…é™¤chunk 2çš„failedçŠ¶æ€ï¼Œè®©å®ƒé‡è¯•
python -c "
import json
with open('data/output/transforms/polyphenol-2X_BATCHED/pipeline_state.json', 'r+') as f:
    state = json.load(f)
    # ä»failed listç§»é™¤chunk 2
    state['failed_chunks'] = []
    # å°†chunk 2çŠ¶æ€æ”¹ä¸ºpending
    state['chunks'][2]['status'] = 'pending'
    f.seek(0)
    json.dump(state, f, indent=2)
    f.truncate()
print('Chunk 2 reset to pending')
"

# é€‰é¡¹2: è·³è¿‡chunk 2ï¼Œå…ˆå®Œæˆå…¶ä»–chunks
# æ‰‹åŠ¨ä¿®æ”¹pipelineä»£ç è·³è¿‡ç‰¹å®šchunk

# é€‰é¡¹3: ä½¿ç”¨chunk 2çš„éƒ¨åˆ†è¾“å‡º
# æ£€æŸ¥products.parquetæ˜¯å¦å¯ç”¨
# å¯èƒ½éœ€è¦æ‰‹åŠ¨åˆ›å»ºSUMMARY.json
```

---

## ç¬¬å…­éƒ¨åˆ†ï¼šæ‰§è¡Œä¼˜å…ˆçº§ä¸æ—¶é—´ä¼°ç®—

### ç«‹å³æ‰§è¡Œï¼ˆ1-2å°æ—¶ï¼‰

1. **è¿è¡ŒComplexity Analysis** (30åˆ†é’Ÿ)
   ```bash
   python scripts/diagnose_chunk_complexity.py
   ```

2. **è¿è¡ŒMemory Trend Analysis** (10åˆ†é’Ÿ)
   ```bash
   python scripts/analyze_memory_trend.py data/.../chunk_002_output/transform.log
   ```

3. **Repository Cleanup Dry Run** (20åˆ†é’Ÿ)
   ```bash
   python scripts/cleanup_repository.py
   # å®¡æŸ¥è¾“å‡º
   ```

### çŸ­æœŸæ‰§è¡Œï¼ˆåŠå¤©ï¼‰

4. **Repository Cleanup Execute** (30åˆ†é’Ÿ)
   ```bash
   python scripts/cleanup_repository.py --execute
   ```

5. **Gitæäº¤ä¸æ¨é€** (30åˆ†é’Ÿ)
   ```bash
   git add .
   git commit -F commit_message.txt
   git push origin feature/batch-transform-pipeline
   ```

6. **Linuxç¯å¢ƒæ­å»º** (2å°æ—¶)
   - SSHç™»å½•
   - å®‰è£…condaå’Œä¾èµ–
   - å…‹éš†ä»“åº“
   - ä¼ è¾“æµ‹è¯•æ•°æ®

### ä¸­æœŸæ‰§è¡Œï¼ˆ1-2å¤©ï¼‰

7. **Linuxå…¼å®¹æ€§æµ‹è¯•** (4å°æ—¶)
   - 100Kæµ‹è¯•
   - Chunk 2å¯¹æ¯”æµ‹è¯•
   - æ€§èƒ½å¯¹æ¯”åˆ†æ

8. **åŸºäºè¯Šæ–­ç»“æœçš„Pipelineä¼˜åŒ–** (4-8å°æ—¶)
   - å®ç°adaptive chunkingï¼ˆå¦‚æœéœ€è¦ï¼‰
   - æˆ–å®ç°dynamic timeout
   - æˆ–å…¶ä»–ä¼˜åŒ–æªæ–½

### é•¿æœŸæ‰§è¡Œï¼ˆ3-7å¤©ï¼‰

9. **å®Œæ•´Pipelineè¿è¡Œ** (å–å†³äºä¼˜åŒ–ç»“æœ)
   - å¦‚æœæ¯chunk 2-3å°æ—¶ï¼š14 chunks Ã— 3h = 42å°æ—¶
   - å¦‚æœä¼˜åŒ–å1-2å°æ—¶ï¼š14 chunks Ã— 2h = 28å°æ—¶
   - å»ºè®®åœ¨Linuxä¸Š24/7è¿è¡Œï¼Œå®šæœŸæ£€æŸ¥

10. **ç»“æœéªŒè¯ä¸åˆå¹¶** (2å°æ—¶)
    - éªŒè¯æ‰€æœ‰chunkså®Œæˆ
    - è¿è¡Œmergeæ“ä½œ
    - è´¨é‡æ£€æŸ¥

---

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæˆåŠŸæ ‡å‡†

### Minimum Viable Product (å¿…é¡»è¾¾æˆ)

- âœ… å®Œæˆæ‰€æœ‰14 chunksçš„å¤„ç†
- âœ… æ¯ä¸ªchunkæœ‰valid SUMMARY.json
- âœ… åˆå¹¶åçš„äº§å“æ•°é‡åˆç†ï¼ˆé¢„è®¡40-60Mï¼‰
- âœ… æ— æ•°æ®æŸåæˆ–ç¼ºå¤±

### Optimal Goals (æœŸæœ›è¾¾æˆ)

- âœ… ç†è§£å¹¶è§£å†³timeoutçš„æ ¹æœ¬åŸå› 
- âœ… å•chunkå¤„ç†æ—¶é—´å¯é¢„æµ‹ï¼ˆåŸºäºå¤æ‚åº¦ï¼‰
- âœ… åœ¨Linuxä¸ŠæˆåŠŸè¿è¡Œå¹¶éªŒè¯æ›´å¥½çš„æ€§èƒ½
- âœ… Repositoryå¹²å‡€æ•´æ´ï¼Œready for production

### Stretch Goals (å¦‚æœ‰æ—¶é—´)

- âœ… å®ç°adaptive chunking
- âœ… å®Œæ•´çš„Linuxéƒ¨ç½²æ–‡æ¡£
- âœ… è‡ªåŠ¨åŒ–monitoring dashboard
- âœ… å°†batch pipelineæ‰©å±•åˆ°terpenoidç­‰å…¶ä»–æ•°æ®é›†

---

## ç¬¬å…«éƒ¨åˆ†ï¼šç´§æ€¥è”ç³»ä¸å›æ»š

### å¦‚æœé‡åˆ°ä¸¥é‡é—®é¢˜

**é—®é¢˜1: Pipelineå¡æ­»æ— å“åº”**

```bash
# æŸ¥æ‰¾å¹¶killè¿›ç¨‹
ps aux | grep batch_transform_pipeline.py
kill -9 <PID>

# æ£€æŸ¥çŠ¶æ€æ–‡ä»¶
cat data/output/transforms/polyphenol-2X_BATCHED/pipeline_state.json

# æ¢å¤ï¼šé‡æ–°è¿è¡Œpipelineï¼ˆä¼šè‡ªåŠ¨resumeï¼‰
```

**é—®é¢˜2: ç£ç›˜ç©ºé—´ä¸è¶³**

```bash
# æ£€æŸ¥ç©ºé—´
df -h

# ç´§æ€¥æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf data/output/transforms/OPT_*
rm -rf data/output/transforms/TEST_*
```

**é—®é¢˜3: Gitæ¨é€å¤±è´¥**

```bash
# æ£€æŸ¥å¤§æ–‡ä»¶
git ls-files | xargs ls -lh | sort -k5 -hr | head -20

# å¦‚æœæœ‰å¤§æ–‡ä»¶è¢«è¯¯åŠ å…¥
git rm --cached <large_file>
git commit --amend
```

### å›æ»šåˆ°å®‰å…¨çŠ¶æ€

```bash
# å¦‚æœcleanupç ´åäº†ä»€ä¹ˆ
git reset --hard HEAD~1  # å›æ»šæœ€åä¸€æ¬¡commit

# å¦‚æœéœ€è¦æ¢å¤æ–‡ä»¶
git checkout HEAD -- <file_path>
```

---

## ç¬¬ä¹éƒ¨åˆ†ï¼šç»™ä¸‹ä¸€ä¸ªä¼šè¯çš„å…·ä½“æŒ‡ä»¤

### ä¼šè¯å¼€å§‹æ—¶çš„First Steps

1. **è¯»å–æœ¬æ–‡æ¡£**ï¼ˆä½ æ­£åœ¨åšï¼‰âœ“

2. **ç¡®è®¤å½“å‰ç¯å¢ƒ**
   ```bash
   pwd  # åº”è¯¥åœ¨ E:/Projects/halogenator
   git branch  # åº”è¯¥åœ¨æŸä¸ªfeatureåˆ†æ”¯
   ls data/output/transforms/polyphenol-2X_BATCHED/pipeline_state.json  # åº”è¯¥å­˜åœ¨
   ```

3. **è¿è¡Œè¯Šæ–­è„šæœ¬**
   ```bash
   python scripts/diagnose_chunk_complexity.py
   ```

4. **åŸºäºè¯Šæ–­ç»“æœå†³å®šä¸‹ä¸€æ­¥**
   - å¦‚æœcomplexityå·®å¼‚å·¨å¤§ â†’ å®ç°adaptive chunking
   - å¦‚æœæ— æ˜æ˜¾å·®å¼‚ â†’ è€ƒè™‘å…¶ä»–åŸå› ï¼ˆå†…å­˜ã€I/Oã€Windowsï¼‰
   - å¦‚æœä¸ç¡®å®š â†’ å…ˆåœ¨Linuxä¸Šæµ‹è¯•

### å†³ç­–æ ‘

```
START
  â”œâ”€â†’ Complexityæ˜æ˜¾ä¸å‡ï¼Ÿ
  â”‚   â”œâ”€ YES â†’ å®ç°adaptive chunking â†’ é‡è·‘pipeline
  â”‚   â””â”€ NO â†’ ç»§ç»­ä¸‹ä¸€æ­¥
  â”‚
  â”œâ”€â†’ æœ‰LinuxæœåŠ¡å™¨å¯ç”¨ï¼Ÿ
  â”‚   â”œâ”€ YES â†’ ç«‹å³è¿ç§»å¹¶æµ‹è¯• â†’ å¯¹æ¯”ç»“æœ
  â”‚   â””â”€ NO â†’ ç»§ç»­Windowsä¼˜åŒ–
  â”‚
  â”œâ”€â†’ å†³å®šç»§ç»­Windowsï¼Ÿ
  â”‚   â”œâ”€ å¢åŠ timeoutåˆ°8å°æ—¶
  â”‚   â”œâ”€ æˆ–å‡å°chunk sizeåˆ°500K
  â”‚   â””â”€ é‡è·‘pipeline
  â”‚
  â””â”€â†’ åœ¨Linuxæµ‹è¯•ï¼Ÿ
      â”œâ”€ æ­å»ºç¯å¢ƒ (2å°æ—¶)
      â”œâ”€ ä¼ è¾“æ•°æ® (1-2å°æ—¶)
      â”œâ”€ è¿è¡Œchunk 2æµ‹è¯• (2-4å°æ—¶)
      â””â”€ åŸºäºç»“æœå†³å®š
```

---

## é™„å½•Aï¼šå®Œæ•´çš„å‘½ä»¤é€ŸæŸ¥è¡¨

### è¯Šæ–­å‘½ä»¤

```bash
# Complexityåˆ†æ
python scripts/diagnose_chunk_complexity.py

# Memoryè¶‹åŠ¿
python scripts/analyze_memory_trend.py data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_002_output/transform.log

# æ£€æŸ¥pipelineçŠ¶æ€
cat data/output/transforms/polyphenol-2X_BATCHED/pipeline_state.json | python -m json.tool

# æ£€æŸ¥æ‰€æœ‰chunksçŠ¶æ€
for i in {0..13}; do
    if [ -f "data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_$(printf '%03d' $i)_output/SUMMARY.json" ]; then
        echo "Chunk $i: DONE"
    else
        echo "Chunk $i: PENDING/FAILED"
    fi
done
```

### Pipelineæ“ä½œ

```bash
# è¿è¡Œpipelineï¼ˆè‡ªåŠ¨resumeï¼‰
python batch_transform_pipeline.py

# æ£€æŸ¥æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹
ps aux | grep python | grep -E "batch_transform|08_transform"

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f data/output/transforms/polyphenol-2X_BATCHED/chunks/chunk_XXX_output/transform.log
```

### Repositoryæ•´ç†

```bash
# Dry run
python scripts/cleanup_repository.py

# æ‰§è¡Œ
python scripts/cleanup_repository.py --execute

# æ‰‹åŠ¨æ•´ç†æ–‡æ¡£
mkdir -p docs/{reports,guides,archive}
mv SESSION_*.md docs/reports/
mv *_GUIDE.md docs/guides/
```

### Gitæ“ä½œ

```bash
# çŠ¶æ€æ£€æŸ¥
git status
git branch

# æäº¤
git add .
git commit -F commit_message.txt

# æ¨é€
git push origin feature/batch-transform-pipeline

# æ ‡ç­¾
git tag -a v0.2.0-batch-pipeline -m "Batch pipeline implementation"
git push origin v0.2.0-batch-pipeline
```

### Linuxè¿ç§»

```bash
# ä¼ è¾“æ•°æ®
rsync -avz --progress /e/Projects/halogenator/data/output/nplike_v2/polyphenol-2X/ user@linux:/path/to/halogenator/data/output/nplike_v2/polyphenol-2X/

# SSHç™»å½•
ssh user@linux-server

# ç¯å¢ƒæ­å»º
conda create -n halo-p0 python=3.9
conda activate halo-p0
pip install -r requirements.txt

# æµ‹è¯•
python scripts/08_transform_library_v2.py --help
```

---

## é™„å½•Bï¼šé¢„æœŸäº§å‡ºæ–‡ä»¶æ¸…å•

### æœ¬ä¼šè¯åº”è¯¥äº§ç”Ÿçš„æ–‡ä»¶

```
scripts/
â”œâ”€â”€ diagnosis/
â”‚   â”œâ”€â”€ diagnose_chunk_complexity.py    â† æ–°å»º
â”‚   â””â”€â”€ analyze_memory_trend.py         â† æ–°å»º
â””â”€â”€ production/
    â””â”€â”€ cleanup_repository.py           â† æ–°å»º

docs/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ SESSION_HANDOFF_BATCH_PIPELINE_AND_CLEANUP.md  â† æœ¬æ–‡æ¡£
â”‚   â””â”€â”€ chunk_complexity_analysis.json  â† è¯Šæ–­è¾“å‡º
â””â”€â”€ guides/
    â””â”€â”€ (æ•´ç†åçš„æ–‡æ¡£)

# Git
.git/
â””â”€â”€ (æ–°commitså’Œtags)

# æ¸…ç†åçš„ç›®å½•ç»“æ„
(è§ä»»åŠ¡Bçš„ç›®æ ‡ç»“æ„)
```

---

## ç»“è¯­

æœ¬ä¼šè¯çš„æ ¸å¿ƒç›®æ ‡æ˜¯**è°ƒæŸ¥timeoutæ ¹æœ¬åŸå› **å¹¶**å®Œæˆrepositoryæ¸…ç†å‡†å¤‡è¿ç§»**ã€‚

**å…³é”®è®°å¿†ç‚¹ï¼š**
1. âš ï¸ **ä¸è¦ç›²ç›®å¢åŠ timeout** - å…ˆè¯Šæ–­æ ¹æœ¬åŸå› 
2. ğŸ” **Chunk 2çš„7.3Mäº§å“æ˜¯å…³é”®çº¿ç´¢** - å¤æ‚åº¦é—®é¢˜
3. ğŸ§ **Linuxæµ‹è¯•æ˜¯critical path** - å¯èƒ½è§£å†³Windowsé—®é¢˜
4. ğŸ—‚ï¸ **æ¸…ç†repositoryæ˜¯è¿ç§»å‰æ** - ä¸è¦å¸¦ç€åƒåœ¾è¿ç§»
5. ğŸ“Š **è¿è¡Œè¯Šæ–­å·¥å…·è·å¾—æ•°æ®** - æ•°æ®é©±åŠ¨å†³ç­–

**æˆåŠŸæ ‡å¿—ï¼š**
- âœ… ç†è§£äº†ä¸ºä»€ä¹ˆchunk 2éœ€è¦>4å°æ—¶
- âœ… Repositoryå¹²å‡€æ•´æ´
- âœ… ä»£ç å·²pushåˆ°Github
- âœ… åœ¨Linuxä¸ŠæˆåŠŸè¿è¡Œæµ‹è¯•
- âœ… æœ‰æ˜ç¡®çš„ä¼˜åŒ–æ–¹æ¡ˆ

**å¦‚æœæ—¶é—´æœ‰é™ï¼Œä¼˜å…ˆçº§ï¼š**
1. è¿è¡Œcomplexityè¯Šæ–­ï¼ˆå¿…é¡»ï¼‰
2. Linuxæµ‹è¯•ï¼ˆå¿…é¡»ï¼‰
3. Repositoryæ¸…ç†ï¼ˆé‡è¦ï¼‰
4. Gitæ¨é€ï¼ˆé‡è¦ï¼‰
5. å®ç°ä¼˜åŒ–æ–¹æ¡ˆï¼ˆåŸºäº1å’Œ2çš„ç»“æœï¼‰

ç¥ä¸‹ä¸€ä¸ªä¼šè¯é¡ºåˆ©ï¼ğŸš€

---

**æ–‡æ¡£ç‰ˆæœ¬:** 1.0
**æœ€åæ›´æ–°:** 2025-12-29
**ä½œè€…:** Claude Sonnet 4.5
**çŠ¶æ€:** Ready for handoff
