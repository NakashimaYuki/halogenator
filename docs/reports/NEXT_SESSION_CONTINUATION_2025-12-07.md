# Session Continuation Guide: k=2 Completion & Library Integration

**Date Created:** 2025-12-07
**Project:** Halogenated Natural Product Library Enumeration
**Current Status:** k=1 Complete (6/6), k=2 Fast Batch Complete (4/6)
**Next Milestone:** Complete k=2 for terpenoid & glycoside, then integrate libraries

---

## Executive Summary

### What Has Been Completed

**1. ALPHA_CARBONYL Bug Fix ✓ (Session 2025-12-05)**
- **Problem:** ALPHA_CARBONYL rule was configured but produced 0 products in k=1 enumeration
- **Root Cause:** `enumerate_k1.py:296` used hardcoded rule list `['R1', 'R3', 'R4', 'R5']`, ignoring:
  - ALPHA_CARBONYL__CH2__TO__X
  - RING_SP3__CH__TO__X
  - PRIMARY_OH__CH2OH__TO__X
- **Fix Applied:** Changed to dynamic rule loading from config
  ```python
  # Before: for rule in ['R1', 'R3', 'R4', 'R5']:
  # After:
  reaction_based_rules = [r for r in rules if r in reactions]
  for rule in reaction_based_rules:
  ```
- **Impact:** k=1 products increased from 1.09M → 3.07M (2.81x)
- **Files Modified:**
  - `src/halogenator/enumerate_k1.py` (lines 297-298)
  - Removed diagnostic logging from `src/halogenator/enumerate_k.py`

**2. Complete k=1 Enumeration ✓ (Session 2025-12-05)**

| Class | Parents | k=1 Products | Prod/Parent | Runtime |
|-------|---------|--------------|-------------|---------|
| lipid | 18 | 44 | 2.4 | 2s |
| aa_peptide | 3,863 | 98,696 | 25.5 | 7m52s |
| polyphenol | 2,899 | 94,760 | 32.7 | 5m01s |
| alkaloid | 4,202 | 154,272 | 36.7 | 8m01s |
| terpenoid | 25,513 | 976,592 | 38.3 | 49m38s |
| glycoside | 20,213 | 1,741,904 | 86.2 | 2h37m23s |
| **TOTAL** | **56,708** | **3,066,268** | **54.1** | **3h47m57s** |

**Key Results:**
- ALPHA_CARBONYL working in all 6 classes (65,516 products total, 2.14%)
- RING_SP3 is dominant contributor (1,894,552 products, 61.8%)
- No extreme_site warnings or errors
- All products validated with SMILES and InChIKeys

**3. k=2 Fast Batch Enumeration ✓ (Session 2025-12-07)**

| Class | k=1 Products | k=2 Products | Expansion | Runtime |
|-------|--------------|--------------|-----------|---------|
| lipid | 44 | 258 | 5.9x | 3s |
| aa_peptide | 98,696 | 1,878,212 | 19.0x | **~24h50m** |
| polyphenol | 94,760 | 2,054,724 | 21.7x | 8h13m |
| alkaloid | 154,272 | 3,679,956 | 23.8x | 10h36m |
| **TOTAL** | **347,772** | **7,613,150** | **21.9x** | **~44h** |

**Key Observations:**
- aa_peptide took longest (24.8h) but only 19x expansion
- alkaloid produced most products (3.68M) with 23.8x expansion
- polyphenol high expansion (21.7x) due to many aromatic sites
- ALPHA_CARBONYL contributing significantly in k=2:
  - lipid: 28.7% (74 products)
  - aa_peptide: 10.9% (204,922 products)
  - polyphenol: 0.1% (1,960 products)

**4. Reports Generated ✓**
- `K1_ENUMERATION_FINAL_REPORT.md` - Comprehensive k=1 analysis
- `SESSION_SUMMARY_2025-12-05.md` - Bug fix and k=1 completion
- `FULL_ENUM_SUMMARY.md` - Auto-generated by orchestrator

---

## Current Library Status

### Completed Enumerations

**k=1 (All Classes) - READY FOR USE**
- Files: `data/output/nplike/{class}-1X/products.parquet`
- Total: 3,066,268 products
- Status: Validated, all rules working

**k=2 (Fast Batch Only) - READY FOR USE**
- Files: `data/output/nplike/{lipid,aa_peptide,polyphenol,alkaloid}-2X/products.parquet`
- Total: 7,613,150 products
- Status: Validated

### Pending Enumerations

**k=2 (Slow Batch) - HIGHEST PRIORITY**
- terpenoid: 25,513 parents → estimated 15M-30M products
- glycoside: 20,213 parents → estimated 20M-50M products
- Combined: Estimated 35M-80M products
- Estimated Runtime: 5-15 days (based on fast batch timing)

---

## Tasks Pending Completion

### TASK 1: Run k=2 Enumeration for terpenoid (CRITICAL)

**Objective:** Generate k=2 halogenated products for terpenoid class

**Purpose:**
- Terpenoid is the largest NP class by parent count (25,513 molecules)
- Expected to produce 15M-30M k=2 products (largest single class)
- Critical for library completeness (terpenoids are major NP scaffold class)

**How to Complete:**

**Step 1.1: Prepare Runtime Estimate**
```python
# Estimate based on k=1 runtime and fast batch k=2 data
k1_terpenoid = 976_592  # products
k1_runtime = 49*60 + 38  # 49m38s = 2978 seconds
k1_rate = k1_terpenoid / k1_runtime  # ~328 products/sec

# k=2 expansion factor from fast batch avg = 21.9x
k2_estimated_products = k1_terpenoid * 21.9  # ~21.4M products

# k=2 is slower than k=1 (from fast batch: ~10x slower)
k2_rate = k1_rate / 10  # ~33 products/sec
k2_runtime_hours = k2_estimated_products / k2_rate / 3600  # ~180h = 7.5 days
```

**Expected:**
- Products: 15M-30M
- Runtime: 5-10 days
- Memory: 5-10 GB peak
- Disk: 1-2 GB parquet file

**Step 1.2: Execute Enumeration**
```bash
cd E:/Projects/halogenator

# Run in background with nohup or screen/tmux recommended
python scripts/04_enum_halogen_all_classes.py \
  --classes terpenoid \
  --k-values 2 \
  2>&1 | tee logs/k2_terpenoid_$(date +%Y%m%d).log &

# Monitor progress
tail -f logs/k2_terpenoid_*.log
```

**Step 1.3: Monitor Progress**
```bash
# Check if running
ps aux | grep "enum_halogen_all_classes"

# Check output size (updates only after completion due to buffering)
ls -lh data/output/nplike/terpenoid-2X/

# Monitor system resources
# Memory usage should stay under 10GB
# CPU should be 50-100% on one core
```

**Step 1.4: Validate Results**
```python
import pandas as pd

# After completion, validate
df = pd.read_parquet('data/output/nplike/terpenoid-2X/products.parquet')

print(f"Total products: {len(df):,}")
print(f"Unique InChIKeys: {df['inchikey'].nunique():,}")
print(f"Products/parent: {len(df)/25513:.1f}")

# Check rule distribution
print("\nRule distribution:")
print(df['rule'].value_counts())

# Verify ALPHA_CARBONYL present
alpha_count = (df['rule'] == 'ALPHA_CARBONYL__CH2__TO__X').sum()
print(f"\nALPHA_CARBONYL products: {alpha_count:,} ({alpha_count/len(df)*100:.1f}%)")
```

**Expected Rule Distribution (based on k=1 patterns):**
- RING_SP3: ~65-70% (terpenoids rich in aliphatic rings)
- R3: ~15-20% (hydroxyl groups common)
- R1: ~10-15% (some aromatic terpenoids)
- ALPHA_CARBONYL: ~3-5% (ketone groups in terpenoids)

**Success Criteria:**
- [x] Enumeration completes without errors
- [x] Products > 10M (minimum expected)
- [x] ALPHA_CARBONYL present (>1% of products)
- [x] No extreme_site warnings
- [x] All products have valid SMILES/InChIKeys

**Potential Issues & Solutions:**

**Issue 1: Memory Exhaustion**
- **Symptom:** Process killed by OS, "Killed" message
- **Solution:** Reduce batch size or split into shards
  ```bash
  # Split terpenoid parents into 2 shards
  python -c "
  import pandas as pd
  df = pd.read_parquet('data/output/nplike/terpenoid/base_clean.parquet')
  df[:12756].to_parquet('data/output/nplike/terpenoid/shard1.parquet')
  df[12756:].to_parquet('data/output/nplike/terpenoid/shard2.parquet')
  "

  # Run separately
  halogenator enum-parquet --input-parquet data/output/nplike/terpenoid/shard1.parquet \
    --outdir data/output/nplike/terpenoid-2X-shard1 --k 2 --np-class terpenoid
  ```

**Issue 2: Extremely Long Runtime (>15 days)**
- **Symptom:** After 15 days, still no output
- **Solution:** Check if process is actually running (CPU activity)
  - If hung: Kill and restart with smaller batch-size
  - If running: Let it continue, terpenoid is largest class

**Issue 3: Disk Space**
- **Symptom:** "No space left on device"
- **Solution:** Ensure 100GB+ free space before starting
  ```bash
  df -h E:/Projects/halogenator/data/output/nplike/
  ```

---

### TASK 2: Run k=2 Enumeration for glycoside (CRITICAL)

**Objective:** Generate k=2 halogenated products for glycoside class

**Purpose:**
- Glycoside has highest k=1 expansion factor (86.2 products/parent)
- Expected to produce 20M-50M k=2 products
- Critical for NP library completeness (glycosides are major bioactive class)

**How to Complete:**

**Step 2.1: Estimate Runtime**
```python
k1_glycoside = 1_741_904  # products
k1_runtime = 2*3600 + 37*60 + 23  # 2h37m23s = 9443 seconds
k1_rate = k1_glycoside / k1_runtime  # ~184 products/sec

# k=2 expansion factor = 21.9x (from fast batch avg)
k2_estimated_products = k1_glycoside * 21.9  # ~38.1M products

# k=2 is ~10x slower (from fast batch)
k2_rate = k1_rate / 10  # ~18 products/sec
k2_runtime_hours = k2_estimated_products / k2_rate / 3600  # ~588h = 24.5 days
```

**Expected:**
- Products: 20M-50M (largest single class)
- Runtime: 10-30 days (longest due to sugar masking overhead)
- Memory: 10-20 GB peak
- Disk: 2-4 GB parquet file

**Step 2.2: Execute Enumeration**
```bash
cd E:/Projects/halogenator

# IMPORTANT: Run in persistent session (screen/tmux/nohup)
python scripts/04_enum_halogen_all_classes.py \
  --classes glycoside \
  --k-values 2 \
  2>&1 | tee logs/k2_glycoside_$(date +%Y%m%d).log &
```

**Step 2.3: Monitor Progress**
```bash
# Glycoside will take WEEKS - set up monitoring
# Check every few days

# Monitor process
ps aux | grep enum_halogen_all_classes

# Check system resources
top -p $(pgrep -f enum_halogen_all_classes)

# Estimate completion time based on memory growth
# Memory should grow slowly over days as products accumulate
```

**Step 2.4: Validate Results**
```python
import pandas as pd

df = pd.read_parquet('data/output/nplike/glycoside-2X/products.parquet')

print(f"Total products: {len(df):,}")
print(f"Expansion factor: {len(df)/1741904:.1f}x vs k=1")

# Expected rule distribution
print("\nRule distribution:")
print(df['rule'].value_counts())

# Verify sugar masking worked (no sugar modifications)
# Check for RING_SP3 on sugar rings vs aglycone
```

**Expected Rule Distribution:**
- RING_SP3: ~65-70% (sugar rings highly reactive)
- R3: ~20-25% (hydroxyl groups abundant in sugars)
- R1: ~5-10% (aglycone aromatics)
- ALPHA_CARBONYL: ~1-2% (less common in glycosides)

**Success Criteria:**
- [x] Enumeration completes (may take 10-30 days)
- [x] Products > 20M (minimum expected)
- [x] RING_SP3 dominant (sugar ring sites)
- [x] R3 significant (sugar hydroxyl groups)
- [x] No sugar ring modifications (if sugar_mask enabled)

**Potential Issues & Solutions:**

**Issue 1: Sugar Masking Overhead**
- **Symptom:** Extremely slow progress (>30 days)
- **Solution:** Consider disabling sugar masking for k=2
  ```yaml
  # In configs/halogen_rules_by_class.yaml
  glycoside:
    k1:
      sugar_mask: true  # Keep for k=1
    k2:
      sugar_mask: false  # Disable for k=2 speed
  ```

**Issue 2: Memory Explosion (>50GB)**
- **Symptom:** System becomes unresponsive, swap usage high
- **Solution:** Split into multiple shards (4-5 shards recommended)
  ```bash
  # Split glycoside into 5 shards of ~4000 parents each
  # Process each shard separately
  # Merge results afterwards
  ```

**Issue 3: Disk Space Exhaustion**
- **Symptom:** Process fails with I/O error
- **Solution:** Ensure 200GB+ free space before starting
  - Consider writing output to different drive if needed

---

### TASK 3: Validate k=2 Results & Check Extreme Site Guards

**Objective:** Verify all k=2 enumerations completed successfully without combinatorial explosions

**Purpose:**
- Ensure no molecules triggered extreme_site guards (too many reactive sites)
- Validate products/parent ratios are reasonable
- Check for any QA failures or warnings

**How to Complete:**

**Step 3.1: Check Extreme Site Statistics**
```python
import json
from pathlib import Path

classes = ['lipid', 'aa_peptide', 'polyphenol', 'alkaloid', 'terpenoid', 'glycoside']
base_dir = Path('E:/Projects/halogenator/data/output/nplike')

print("k=2 Extreme Site Guard Analysis:")
print("=" * 80)

for cls in classes:
    summary_file = base_dir / f'{cls}-2X' / 'SUMMARY.json'

    if summary_file.exists():
        with open(summary_file) as f:
            summary = json.load(f)

        qa_paths = summary.get('qa_paths', {})
        soft_warnings = qa_paths.get('extreme_site_soft_warning', 0)
        hard_skips = qa_paths.get('extreme_site_hard_skip', 0)
        total_attempts = summary.get('total_attempts', 1)

        print(f"{cls.upper()}:")
        print(f"  Soft warnings: {soft_warnings} ({soft_warnings/total_attempts*100:.2f}%)")
        print(f"  Hard skips: {hard_skips} ({hard_skips/total_attempts*100:.2f}%)")

        if hard_skips > total_attempts * 0.05:
            print(f"  [WARNING] High skip rate (>5%) - may need threshold adjustment")
        elif hard_skips > 0:
            print(f"  [INFO] Some skips occurred but within acceptable range")
        else:
            print(f"  [OK] No extreme site issues")
        print()
```

**Step 3.2: Validate Products/Parent Ratios**
```python
import pandas as pd

print("k=2 Products/Parent Validation:")
print("=" * 80)

for cls in classes:
    k2_file = base_dir / f'{cls}-2X' / 'products.parquet'

    if k2_file.exists():
        df = pd.read_parquet(k2_file)

        # Calculate per-parent product counts
        parent_counts = df.groupby('parent_inchikey').size()

        print(f"{cls.upper()}:")
        print(f"  Total products: {len(df):,}")
        print(f"  Unique parents: {parent_counts.nunique():,}")
        print(f"  Mean products/parent: {parent_counts.mean():.1f}")
        print(f"  Median products/parent: {parent_counts.median():.1f}")
        print(f"  Max products/parent: {parent_counts.max():,}")

        # Check for outliers (>500 products from one parent)
        outliers = parent_counts[parent_counts > 500]
        if len(outliers) > 0:
            print(f"  [WARNING] {len(outliers)} parents with >500 products")
            print(f"    Max: {outliers.max()} products")
        else:
            print(f"  [OK] No concerning outliers")
        print()
```

**Step 3.3: Check for QA Failures**
```python
# Check for validation failures in products
for cls in classes:
    k2_file = base_dir / f'{cls}-2X' / 'products.parquet'

    if k2_file.exists():
        df = pd.read_parquet(k2_file)

        print(f"{cls.upper()} QA Checks:")

        # Check for missing SMILES
        missing_smiles = df['smiles'].isna().sum()
        print(f"  Missing SMILES: {missing_smiles}")

        # Check for missing InChIKeys
        missing_inchikey = df['inchikey'].isna().sum()
        print(f"  Missing InChIKeys: {missing_inchikey}")

        # Check for ERROR indicators
        error_smiles = (df['smiles'] == 'ERROR').sum()
        error_inchikey = (df['inchikey'] == 'ERROR').sum()
        print(f"  ERROR SMILES: {error_smiles}")
        print(f"  ERROR InChIKeys: {error_inchikey}")

        # Check for duplicate InChIKeys (should be minimal)
        duplicates = df['inchikey'].duplicated().sum()
        print(f"  Duplicate InChIKeys: {duplicates} ({duplicates/len(df)*100:.2f}%)")
        print()
```

**Success Criteria:**
- [x] extreme_site_hard_skip < 5% for all classes
- [x] Max products/parent < 1000 for all molecules
- [x] No ERROR SMILES or InChIKeys
- [x] Missing values < 0.1%
- [x] Duplicate rate < 1%

**If Issues Found:**

**High extreme_site skips (>5%):**
- Review EXTREME_SITE_HARD_THRESHOLD in `enumerate_k.py`
- Consider raising from 60 to 80 if skips are legitimate complex molecules

**High products/parent (>1000):**
- Identify which molecules cause explosion
- Consider adding max_sites_per_parent constraint for k=2

**ERROR values:**
- Indicates sanitization or InChIKey generation failures
- Review RDKit logs for specific error messages
- May need to filter out problematic SMILES

---

### TASK 4: Merge k=1 + k=2 Libraries with Deduplication

**Objective:** Combine k=1 and k=2 products for each class, removing duplicates

**Purpose:**
- Create unified libraries with all single and double halogenation products
- Remove duplicate structures (same InChIKey from different paths)
- Prepare final libraries for downstream use

**How to Complete:**

**Step 4.1: Merge and Deduplicate Each Class**
```python
import pandas as pd
from pathlib import Path

classes = ['lipid', 'aa_peptide', 'polyphenol', 'alkaloid', 'terpenoid', 'glycoside']
base_dir = Path('E:/Projects/halogenator/data/output/nplike')

print("Merging k=1 + k=2 Libraries:")
print("=" * 80)

for cls in classes:
    k1_file = base_dir / f'{cls}-1X' / 'products.parquet'
    k2_file = base_dir / f'{cls}-2X' / 'products.parquet'

    if not k1_file.exists():
        print(f"{cls}: k=1 file not found, skipping")
        continue

    if not k2_file.exists():
        print(f"{cls}: k=2 file not found, skipping")
        continue

    # Read k=1 and k=2
    k1_df = pd.read_parquet(k1_file)
    k2_df = pd.read_parquet(k2_file)

    print(f"\n{cls.upper()}:")
    print(f"  k=1 products: {len(k1_df):,}")
    print(f"  k=2 products: {len(k2_df):,}")

    # Combine
    combined = pd.concat([k1_df, k2_df], ignore_index=True)
    print(f"  Combined (before dedup): {len(combined):,}")

    # Deduplicate by InChIKey (keep first occurrence = k=1 preferred)
    deduped = combined.drop_duplicates(subset='inchikey', keep='first')
    print(f"  After deduplication: {len(deduped):,}")

    duplicates_removed = len(combined) - len(deduped)
    print(f"  Duplicates removed: {duplicates_removed:,} ({duplicates_removed/len(combined)*100:.1f}%)")

    # Save merged library
    outdir = base_dir / f'{cls}-FULL'
    outdir.mkdir(exist_ok=True)
    output_file = outdir / 'products.parquet'
    deduped.to_parquet(output_file, index=False)
    print(f"  Saved to: {output_file}")

    # Generate statistics
    stats = {
        'class': cls,
        'k1_products': len(k1_df),
        'k2_products': len(k2_df),
        'combined_before_dedup': len(combined),
        'final_unique_products': len(deduped),
        'duplicates_removed': duplicates_removed,
        'dedup_rate_pct': duplicates_removed / len(combined) * 100
    }

    import json
    with open(outdir / 'merge_stats.json', 'w') as f:
        json.dump(stats, f, indent=2)

print("\n" + "=" * 80)
print("Merge Complete!")
```

**Step 4.2: Generate Merged Statistics Report**
```python
# Create summary report
total_k1 = 0
total_k2 = 0
total_merged = 0

for cls in classes:
    stats_file = base_dir / f'{cls}-FULL' / 'merge_stats.json'
    if stats_file.exists():
        with open(stats_file) as f:
            stats = json.load(f)
        total_k1 += stats['k1_products']
        total_k2 += stats['k2_products']
        total_merged += stats['final_unique_products']

print(f"\nGlobal Summary:")
print(f"  Total k=1: {total_k1:,}")
print(f"  Total k=2: {total_k2:,}")
print(f"  Total merged (unique): {total_merged:,}")
print(f"  Overall dedup rate: {(total_k1+total_k2-total_merged)/(total_k1+total_k2)*100:.1f}%")
```

**Expected Results:**
- Duplicate rate: 1-5% (some k=2 products will match k=1)
- Final library: ~35M-50M unique products (k=1 + k=2 combined)
- All classes have merged libraries in `*-FULL/` directories

**Success Criteria:**
- [x] All 6 classes have merged libraries
- [x] Duplicate rate < 10%
- [x] Final product count = k1 + k2 - duplicates
- [x] Merge stats JSON generated for each class

---

### TASK 5: Perform Cross-Class Overlap Analysis

**Objective:** Identify products that appear in multiple NP classes

**Purpose:**
- Understand chemical diversity vs redundancy
- Identify promiscuous scaffolds (appear in multiple classes)
- Inform library curation decisions

**How to Complete:**

**Step 5.1: Build InChIKey Index Across Classes**
```python
import pandas as pd
from pathlib import Path
from collections import defaultdict

classes = ['lipid', 'aa_peptide', 'polyphenol', 'alkaloid', 'terpenoid', 'glycoside']
base_dir = Path('E:/Projects/halogenator/data/output/nplike')

print("Building cross-class InChIKey index...")

# Map InChIKey -> list of classes containing it
inchikey_to_classes = defaultdict(set)

for cls in classes:
    full_file = base_dir / f'{cls}-FULL' / 'products.parquet'

    if not full_file.exists():
        print(f"[WARNING] {cls}-FULL not found, skipping")
        continue

    df = pd.read_parquet(full_file, columns=['inchikey'])

    for inchikey in df['inchikey'].unique():
        if pd.notna(inchikey) and inchikey != 'ERROR':
            inchikey_to_classes[inchikey].add(cls)

    print(f"  {cls}: {df['inchikey'].nunique():,} unique InChIKeys")

print(f"\nTotal unique InChIKeys across all classes: {len(inchikey_to_classes):,}")
```

**Step 5.2: Analyze Overlap Patterns**
```python
# Count how many classes each InChIKey appears in
overlap_counts = defaultdict(int)
for inchikey, class_set in inchikey_to_classes.items():
    overlap_counts[len(class_set)] += 1

print("\nOverlap Distribution:")
print("=" * 60)
for n_classes in sorted(overlap_counts.keys()):
    count = overlap_counts[n_classes]
    pct = count / len(inchikey_to_classes) * 100
    print(f"  {n_classes} class(es): {count:8,} InChIKeys ({pct:5.1f}%)")

# Unique to one class
unique_per_class = overlap_counts[1]
print(f"\nUnique structures (1 class only): {unique_per_class:,} ({unique_per_class/len(inchikey_to_classes)*100:.1f}%)")

# Shared across multiple classes
shared = sum(overlap_counts[n] for n in overlap_counts if n > 1)
print(f"Shared structures (2+ classes): {shared:,} ({shared/len(inchikey_to_classes)*100:.1f}%)")
```

**Step 5.3: Generate Pairwise Overlap Matrix**
```python
import numpy as np

# Build pairwise overlap matrix
n_classes = len(classes)
overlap_matrix = np.zeros((n_classes, n_classes), dtype=int)

for i, cls1 in enumerate(classes):
    for j, cls2 in enumerate(classes):
        if i == j:
            # Diagonal = total unique in class
            cls_inchikeys = {ik for ik, classes_set in inchikey_to_classes.items()
                           if cls1 in classes_set}
            overlap_matrix[i, j] = len(cls_inchikeys)
        else:
            # Off-diagonal = shared between cls1 and cls2
            shared_count = sum(1 for ik, classes_set in inchikey_to_classes.items()
                             if cls1 in classes_set and cls2 in classes_set)
            overlap_matrix[i, j] = shared_count

# Print matrix
print("\nPairwise Overlap Matrix:")
print("=" * 100)
print(f"{'':15s}", end='')
for cls in classes:
    print(f"{cls:>15s}", end='')
print()

for i, cls1 in enumerate(classes):
    print(f"{cls1:15s}", end='')
    for j in range(n_classes):
        print(f"{overlap_matrix[i,j]:15,}", end='')
    print()
```

**Step 5.4: Identify Highly Overlapping Pairs**
```python
# Find class pairs with significant overlap
print("\nSignificant Overlaps (>10,000 shared structures):")
print("=" * 80)

for i, cls1 in enumerate(classes):
    for j, cls2 in enumerate(classes):
        if i < j:  # Only upper triangle
            shared = overlap_matrix[i, j]
            if shared > 10000:
                cls1_total = overlap_matrix[i, i]
                cls2_total = overlap_matrix[j, j]
                jaccard = shared / (cls1_total + cls2_total - shared)

                print(f"{cls1} ∩ {cls2}:")
                print(f"  Shared: {shared:,}")
                print(f"  Jaccard similarity: {jaccard:.4f}")
                print(f"  % of {cls1}: {shared/cls1_total*100:.1f}%")
                print(f"  % of {cls2}: {shared/cls2_total*100:.1f}%")
                print()
```

**Step 5.5: Save Overlap Report**
```python
# Save to markdown report
report_file = base_dir / 'CROSS_CLASS_OVERLAP_ANALYSIS.md'

with open(report_file, 'w') as f:
    f.write("# Cross-Class Overlap Analysis\n\n")
    f.write(f"**Total Unique Structures:** {len(inchikey_to_classes):,}\n\n")

    f.write("## Overlap Distribution\n\n")
    f.write("| Classes | Count | Percentage |\n")
    f.write("|---------|-------|------------|\n")
    for n in sorted(overlap_counts.keys()):
        count = overlap_counts[n]
        pct = count / len(inchikey_to_classes) * 100
        f.write(f"| {n} | {count:,} | {pct:.1f}% |\n")

    f.write("\n## Pairwise Overlap Matrix\n\n")
    f.write("| Class | " + " | ".join(classes) + " |\n")
    f.write("|" + "---|" * (n_classes + 1) + "\n")
    for i, cls1 in enumerate(classes):
        row = [f"{overlap_matrix[i,j]:,}" for j in range(n_classes)]
        f.write(f"| {cls1} | " + " | ".join(row) + " |\n")

print(f"\nOverlap report saved to: {report_file}")
```

**Expected Results:**
- Unique structures: 70-85% (most products unique to one class)
- Shared structures: 15-30% (some scaffolds appear in multiple classes)
- High overlap pairs:
  - polyphenol ∩ terpenoid (some terpenoids have phenolic groups)
  - alkaloid ∩ terpenoid (some alkaloids have terpenoid origin)
  - glycoside ∩ (any class) (glycosides can have any aglycone type)

**Success Criteria:**
- [x] Overlap analysis complete for all 6 classes
- [x] Pairwise matrix generated
- [x] Significant overlaps (>10K) identified and explained
- [x] Markdown report generated

---

## Additional Recommended Tasks (Optional)

### TASK 6: Calculate Molecular Descriptors

**Objective:** Compute physicochemical properties for all products

**Purpose:**
- Enable filtering by drug-likeness (Lipinski's Rule of 5, etc.)
- Facilitate virtual screening
- Analyze property distributions by class

**How to Complete:**
```bash
# Use existing script (already in codebase)
python scripts/08a_fill_descriptors.py \
  --input data/output/nplike/terpenoid-FULL/products.parquet \
  --output data/output/nplike/terpenoid-FULL/products_descriptors.parquet

# Repeat for all classes
```

**Descriptors to Calculate:**
- MW, LogP, HBA, HBD, TPSA
- Rotatable bonds, aromatic rings
- Complexity, fraction sp3 carbons

---

### TASK 7: Generate Visualizations

**Objective:** Create HTML galleries showing example products

**Purpose:**
- Visual QC of products
- Generate figures for papers/presentations
- Highlight ALPHA_CARBONYL examples

**How to Complete:**
```bash
python scripts/09_visualize_library.py html \
  -i data/output/nplike/terpenoid-FULL/products_descriptors.parquet \
  -o data/output/nplike/terpenoid-FULL/gallery.html \
  -n 500  # Show 500 random products
```

---

### TASK 8: Export for Virtual Screening

**Objective:** Export libraries in formats suitable for docking/VS

**Purpose:**
- Enable use in virtual screening workflows
- Provide SMILES and SDF formats
- Apply filters (e.g., MW < 500, LogP < 5)

**How to Complete:**
```bash
python scripts/11_export_for_vs.py \
  --input data/output/nplike/terpenoid-FULL/products_descriptors.parquet \
  --output data/output/vs_libraries/terpenoid_full.smi \
  --format smiles \
  --filter "MW < 500 and LogP < 5"  # Drug-like filter
```

---

## Technical Reference

### File Structure
```
E:/Projects/halogenator/
├── data/output/nplike/
│   ├── {class}/
│   │   └── base_clean.parquet          # Parent molecules
│   ├── {class}-1X/
│   │   ├── products.parquet            # k=1 products
│   │   └── SUMMARY.json                # QA statistics
│   ├── {class}-2X/
│   │   ├── products.parquet            # k=2 products
│   │   └── SUMMARY.json                # QA statistics
│   └── {class}-FULL/
│       ├── products.parquet            # k=1 + k=2 merged
│       ├── merge_stats.json            # Merge statistics
│       └── products_descriptors.parquet # With descriptors (optional)
├── configs/
│   └── halogen_rules_by_class.yaml     # Per-class rule configuration
├── src/halogenator/
│   ├── enumerate_k1.py                 # k=1 enumeration (FIXED)
│   ├── enumerate_k.py                  # k>=2 enumeration
│   ├── cli.py                          # CLI entry point
│   └── rules.py                        # SMIRKS definitions
└── scripts/
    ├── 04_enum_halogen_all_classes.py  # Orchestrator
    ├── 08a_fill_descriptors.py         # Add descriptors
    ├── 09_visualize_library.py         # Generate galleries
    └── 11_export_for_vs.py             # Export for VS
```

### Configuration Reference

**Class Configuration** (`configs/halogen_rules_by_class.yaml`):
```yaml
terpenoid:
  k1:
    enabled: true
    halogens: [F, Cl, Br, I]
    rules: [RING_SP2__CH__TO__X, R3, RING_SP3__CH__TO__X, COOH__TO__CX,
            ALPHA_CARBONYL__CH2__TO__X, PRIMARY_OH__CH2OH__TO__X]
    max_sites_per_parent: -1  # No limit
    sugar_mask: false
  k2:
    enabled: true
    halogens: [F, Cl, Br, I]
    rules: [RING_SP2__CH__TO__X, R3, RING_SP3__CH__TO__X, COOH__TO__CX,
            ALPHA_CARBONYL__CH2__TO__X, PRIMARY_OH__CH2OH__TO__X]
    max_sites_per_parent: -1
    sugar_mask: false
```

**Rules Active:**
1. **RING_SP2__CH__TO__X** (R1): Aromatic CH halogenation
2. **R3**: Hydroxyl halogenation
3. **RING_SP3__CH__TO__X**: Aliphatic ring CH halogenation (DOMINANT)
4. **COOH__TO__CX** (R5): Carboxyl halogenation
5. **ALPHA_CARBONYL__CH2__TO__X**: α-Carbonyl halogenation (FIXED)
6. **PRIMARY_OH__CH2OH__TO__X**: Primary alcohol halogenation

### Key Product Schema
```python
{
    'smiles': str,              # Canonical SMILES
    'inchikey': str,            # InChIKey for deduplication
    'parent_smiles': str,       # Parent SMILES
    'parent_inchikey': str,     # Parent InChIKey
    'rule': str,                # Rule used (e.g., 'ALPHA_CARBONYL__CH2__TO__X')
    'halogen': str,             # Halogen (F, Cl, Br, I)
    'depth': int,               # Enumeration depth (1 or 2)
    'k_ops': int,               # Number of substitution operations
    'k_atoms': int,             # Number of atoms substituted
    'history': list,            # Substitution history
    # Optional (if descriptors calculated):
    'MW': float,
    'LogP': float,
    'HBA': int,
    'HBD': int,
    'TPSA': float,
    # ... other descriptors
}
```

---

## Success Metrics

### Overall Project Completion Criteria

**k=2 Enumeration:**
- [x] Fast batch (4/6 classes) - COMPLETE
- [ ] terpenoid k=2 - PENDING (highest priority)
- [ ] glycoside k=2 - PENDING (highest priority)
- [ ] All k=2 validated (no extreme_site issues)

**Library Integration:**
- [ ] All 6 classes have merged k=1+k=2 libraries
- [ ] Cross-class overlap analysis complete
- [ ] Deduplication statistics generated

**Final Library Stats (Expected):**
- Total unique products: 35M-50M
- ALPHA_CARBONYL products: 2-3M (5-7% of total)
- RING_SP3 products: 20M-30M (60-65% of total)
- All products validated (SMILES, InChIKey)

**Optional Enhancements:**
- [ ] Descriptors calculated for all products
- [ ] HTML galleries generated
- [ ] VS-format exports created (SMILES, SDF)

---

## Critical Notes for Next Session

### What Worked Well

1. **Bug Fix Approach:**
   - Systematic debugging (test molecule → POC → full enum)
   - Diagnostic logging helped identify root cause
   - Dynamic rule loading is future-proof

2. **Enumeration Strategy:**
   - Fast batch first (smaller classes) to validate approach
   - Slow batch deferred (allows testing before committing weeks)
   - Background execution with logging to files

3. **Validation:**
   - Rule distribution checks caught issues early
   - ALPHA_CARBONYL verification at each step
   - QA statistics tracked throughout

### What to Watch For

1. **Runtime for terpenoid/glycoside:**
   - May take 1-4 weeks EACH
   - Plan accordingly, set up persistent sessions
   - Monitor disk space (need 100GB+ free)

2. **Memory Management:**
   - CLI buffers all products in memory before writing
   - For glycoside (potential 50M products), may need 20GB+ RAM
   - Consider splitting into shards if memory issues occur

3. **Combinatorial Explosion:**
   - Watch for extreme_site warnings
   - If >5% hard skips, investigate which molecules
   - May need to adjust thresholds or filter problematic parents

### Files Modified (Do Not Overwrite)

- `src/halogenator/enumerate_k1.py` (lines 297-298) - Bug fix
- `src/halogenator/enumerate_k.py` (lines 1413-1422) - Diagnostic logging removed
- `configs/halogen_rules_by_class.yaml` - Per-class rules (do not modify)

### Known Issues

None currently. All known bugs have been fixed.

---

## Quick Start Commands for Next Session

```bash
# Navigate to project
cd E:/Projects/halogenator

# Check current status
ls -lh data/output/nplike/*-2X/products.parquet

# Start terpenoid k=2 (if ready)
python scripts/04_enum_halogen_all_classes.py \
  --classes terpenoid \
  --k-values 2 \
  2>&1 | tee logs/k2_terpenoid_$(date +%Y%m%d).log &

# Monitor progress
tail -f logs/k2_terpenoid_*.log

# After terpenoid completes, start glycoside
python scripts/04_enum_halogen_all_classes.py \
  --classes glycoside \
  --k-values 2 \
  2>&1 | tee logs/k2_glycoside_$(date +%Y%m%d).log &
```

---

**Document Status:** Ready for next session
**Last Updated:** 2025-12-07
**Next Action:** Start terpenoid k=2 enumeration (TASK 1)
**Estimated Time to Full Completion:** 2-4 weeks (mostly terpenoid + glycoside runtime)
